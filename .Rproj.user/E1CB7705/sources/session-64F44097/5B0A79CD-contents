---
title: "Social Behaviour Dynamics: Week 5"
author: "Ellen Hamaker"
date: "ADS, 2021-2022"
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output:
  html_document:
    highlight: default
    theme: paper
    toc: yes
    toc_float: yes
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: '5'
params:
  answers: true
  rcode: true
---
# 1. Readings 

There are three papers as reading materials for this week; for each of them there is an R exercise to get hands-on experience. Hence, you do not have to read everything first before moving to the exercises. 

Instead, you can start with reading Pearl, and then do the R-exercise on Pre-post test design; then read Kim and Steiner and do the exercise on unmeasured confounding; and then read VanderWeele et al., and do the exercise about time-varying treatment. 

Please also note that for each paper it is indicated which parts you can (should!) skip. 

$~$

$\blacktriangleright$ Pearl, J. (2016). Lord's paradox revisited - (Oh Lord! Kumbaya!). _Journal of Causal Inference, 4_(2). https://doi.org/10.1515/jci-2016-0021 

__Skip section 6: "The Birth Weight Paradox" (p.11-13)__

After reading Pearl, you can do Exercise 2. Reading questions to Pearl:

1) In the lecture we talk about the treatment X, the pre-test Y1, the post-test Y2, and the gain scrore G=Y2-Y1. Make a little overview for yourself that relates these to the characters that Pearl uses for them (e.g., S, Wi, etc.). 

2) Pearl explains that in the example of Lord, the pretest is actually a mediator; why would one want to control for a mediator, and/or why would one not want to control for a mediator?

3) On p.6, Pearl refers to Wright and Pearl and states the total effect can be obtained by taking the products of the coefficients along each path, and adding these up. When considering Figure 2b, there are in total 4 paths from treatment S to the gain score Y; how do these give the result TE= b - a(1-c) that Pearl reports?

4) In Pearl's example around Figures 4 and 5, the roles of pretest and treatment are reversed. How does this change the problem?

$~$

$\blacktriangleright$ Kim, Y. & Steiner, P. M. (2019). Gain scores revisited: A graphical models perspective. _Sociological Methods & Research_. https://doi.org/10.1177/0049124119826155 

__Skip the section "Advantages of Gain Score Estimators" (p.9-12)__

After reading K&S, you can do Exercise 3. Reading questions to K&S:

1) Relate the notation used by Kim and Steiner, to that uses in the lecture and in the paper by Pearl (e.g., treatment in the lecture is denoted by X, in the first example of Pearl it is denoted by S, and in Kim and Steiner it is Z). Also make sure you see the connection between Figure 2(A) from K&S and the figures regarding unmeasured counfounding from the lecture.

2) The elegance of the chance score analysis is based on the fact that under particular assumptions it ensures that two backdoor paths cancel each other out. What are the assumptions?

$~$

$\blacktriangleright$ VanderWeele, T. J., Jackson, J. W., & Li, S. (2016). Causal inference and longitudinal data: A case study of religion and mental health. _Social Psychiatry and Psychiatric Epidemiology, 51_(11), 1457-1466. https://doi.org/10.1007/s00127-016-1281-9 

__You may skip the section "Hierarchy of evidence for causality across study designs" (p.1461); skip the sections "Results: empirical illustrations" and "Discussion and conclusions" (p.1463-1465)__

After reading VJ&K, you can do Exercise 4. Reading questions to VJ&L:

1) In this paper, they consider the effect of a time-varying treatment A(t) (which is thus measured at multiple occasions) on a distal outcome Y, while controlling for a time-varying covariate L(t). Note that these time-varying covariates can also be prior measurements of the same variable as Y (this is how it is presented in the lecture slides, but the presentation by VJ&L is more general). Make sure you see how the notation and graphs presented here relate to the notation and graphs used in the lecture.

2) What exactly is "time-dependent confounding"? When can we use our standard regression-based methods, and when should we use another technique, according to VJ&L? (see p. 1459) 




# 2. R based on Pearl
This exercise will help you to better understanding the point that __Pearl (2016)__ makes about whether pretest Y1 is a mediator or a confounder. In this exercise you will practice with different scenarios (or data generating mechanisms) that give rise to data in which there is an outcome variable Y2, a (binary) treatment variable X, and a covariate Y1 that is a previous measurement of the same variable as the outcome variable. 

The scenarios under which we generate data are:

* a randomized controlled trial (RCT), in which treatment assignment X is independent of pretest Y1

* a scenario in which treatment groups X already existed prior to the pretest Y1, such that the pretest is a mediator

* a scenario in which treatment assignment X is entirely based on the pretest score Y1, such that the pretest is a confounder

* a scenario in which treatment assignment X is partly based on the pretest score Y1, such that the pretest is a confounder

In  each scenario, the data for the outcome variables Y2 are simulated using the regression model: $Y_{2i} = \beta_0 + \beta_1 X_i + \beta_2 Y_{1i} + e_i$. The _key difference_ between these scenarios is in _how_ and _why_ treatment X and pretest Y1 are related. 

For each dataset, you will then estimate four models:

* an ANCOVA model with the posttest score Y2 as the outcome: $Y_{2i} = \beta_0 + \beta_1 X_i + \beta_2 Y_{1i} + e_i$

* an ANCOVA model with the gain score G=Y2-Y1 as the outcome: $Y_{2i} - Y_{1i} = \beta_0 + \beta_1 X_i + (\beta_2-1) Y_{1i} + e_i$

* a change score model with only $X$ as a predictor: $Y_{2i} - Y_{1i} = \gamma_0 + \gamma_1 X_i + e_i$

* a marginal model in which Y1 is not included: $Y_{2i} = \phi_0 + \phi_1 X_i + e_i$

## 2.1 RCT

We begin with simulating data according to an RCT. This implies that treatment X and pretest Y1 are independent of each other, while the posttest Y2 may depend on both.

The DAG for this model looks like this:

![_Fig.1a: DAG of the RCT model_](ANCOVA_DAG1.jpg){width=25%}

The DAG for this model can be extended to include the gain score G=Y2-Y1:

![_Fig.1b: DAG of the RCT model with change score included_](CHANGE_DAG1.jpg){width=25%}


### 2.1.1 Simulate data
We first need to simulate X (treatment) and Y1 (pretest). Note that in an RCT these will be unrelated (as is also clear from the DAG). 

$\blacktriangleright$ Use the following code to simulate the data of an RCT. 

```{r blokje1a, echo=T, eval=T, message=F}
set.seed(482) # set the seed for comparison

N <- 500		  # total sample size

# For Y1
mY1 <- 115    # mean on pretest
sdY1 <- 20    # sd on pretest
Y1 <- rnorm(N,mY1,sdY1)   # simulate Y1

# For X
X <- rbinom(N,1,0.5)      # simulate treatment

# For Y2
b0 <- 70		  # intercept
b1 <- 20		  # causal effect		
b2 <- .4		  # covariate effect
sdE2 <- 5		  # within-group residual sd
Y2 <- b0 + b1*X + b2*Y1 + rnorm(N,0,sdE2)

dat1 <- data.frame(Y1,Y2,X)
```

$\blacktriangleright$ Consider the correlations between Y1, Y2, and X; what can you say about these? 

```{r blokje1aa, echo=T, eval=T, message=F, include=params$rcode}
round(cor(dat1),3)
```

```{r blokje1aaa, echo=T, eval=T, message=F, include=params$answers}
# Note that Y1 and Y2 are continuous variables, while X is dichotomous.
# To determine the correlation between these three variables, we can 
# use Pearson's correlations.
#
# It shows that:
#   a) Y1 and Y2 are strongly positively related
#   b) X and Y2 are strongly positively related
#   c) Y1 and X are not related (as expected)

```



### 2.1.2 Plot the data

Now we will make plots of the data like the ones used in the lecture (and the literature).

$\blacktriangleright$ First create a scatter plot with Y1 on the x-axis and Y2 on the y-axis, and with separate colors for the two treatment groups. Indicate for all the parameters defined above (mY1, sdY1, b0, b1, b2, sdE2) what feature in the plot they represent. 

```{r blokje1c, echo=T, eval=T, message=F, include=params$rcode}
# For plotting it may be useful to find the overall minimum 
# and maximum across the pretest and posttest, to make the two
# axes comparable
minY <- min(c(Y1,Y2))
maxY <- max(c(Y1,Y2))

plot(x=dat1$Y1, y=dat1$Y2, col = (dat1$X+1),
	    xlim=c(minY,maxY),ylim=c(minY,maxY),
	    xlab="Y1", ylab="Y2")
```

```{r blokje1cc, echo=T, eval=T, message=F, include=params$answers}

# mY1 is the mean on the x-axis
# sdY1 is the variability on the x-axis
# b0 is the intercept of the regression line (not shown) 
# of the no treatment group (when X=0; in  black)
# b1 is the difference in intercepts of regression lines 
# (not shown) of the the treatment group (X=1; in red)
# and the non-treatment group (X=0); it is equal to the ACE
# b2 is the slope of the regression line in each group
# (i.e., the effect of pretest on posttest)
# sdE2 is the residual variability around these regression lines
```


$\blacktriangleright$ Which part of this plot is reflecting the causal effect from the ANCOVA model?

```{r blokje1d, echo=T, eval=T, message=F, include=params$answers}
# The difference in intercept between the two groups, or:
# the distance between the parallel regression lines of the two groups
```

$\blacktriangleright$ Second, make a plot of the means of the pretest scores and post test scores of each group. In the plot, place the two time points on the x-axis and the means of the two groups separately at each time point on the y-axis. Connect the means that belong to the same group using a line plot, and use different colors for each group. 

```{r blokje1e, echo=T, eval=T, message=F, include=params$rcode}
# Means of:
# - pretest non-treatment group (mY10)
# - pretest treatment group (mY11)
# Note that both should be  about mY1=115 (value used in simulation)
mY10 <- mean(Y1[X==0]) 
mY11 <- mean(Y1[X==1]) 

# Means of:
# - posttest non-treatement group (mY20)
# - posttest treatment group (mY21)
# Note that the first should be about:
# mY20 = b0 + b2*mY10 = 70 + 0.4*115 = 116
# and the second should be about:
# mY21 = b0 + b1 + b2*mY11 = 70 + 20 + 0.4*115 = 136
mY20 <- mean(Y2[X==0]) 
mY21 <- mean(Y2[X==1]) 

# Gather means on both occasions per treatment condition
mYX0 <- c(mY10,mY20)
mYX1 <- c(mY11,mY21)

minY <- min(mYX0,mYX1)
maxY <- max(mYX0,mYX1)

plot(c(1,2), mYX0, type="l", 
			xlim=c(0.7,2.3),
			ylim=c(minY-5,maxY+5),xaxt="n",
			xlab="time",
			ylab="Y")
lines(c(1,2),mYX1,col="red")
points(c(1,2),mYX1,pch=19,cex=1.3,col="red")
points(c(1,2),mYX0,pch=19,cex=1.3)
axis(side=1, at=seq(1, 2, by=1))

```

$\blacktriangleright$ How can this plot be used to determine whether in a change score model we would find evidence for a causal effect?

```{r blokje1f, echo=T, eval=T, message=F, include=params$answers}
# A causal effect of X on the gain score (G=Y2-Y1) shows up as a
# different distance between the two lines (groups) at the two occasions;
# this is referred to as differenc-in-differences.
# Here there is no difference at first time point (due to random assignment!),
# whereas there is a 20 point difference at the second time point; this 
# indicates there is a causal effect of X on Y2. 
# Note furher that when no treatment is given, there is hardly any
# change between the means on pretest and posttest.
```

### 2.1.3 Analyze the data
Next, we analyze the data using the four models. 

$\blacktriangleright$  First, estimate an ANCOVA (e.g., using the function `lm()`), with Y2 as the outcome, X as the grouping variable, and Y1 as the covariate (that is: X and Y1 are both predictors in your regression). Report the parameter (estimate, test, p-value) that is relevant for the causality question.

```{r blokjeg, echo=T, eval=T, message=F, include=params$rcode}
ANCOVA <- lm(Y2 ~ X + Y1, data=dat1)
summary(ANCOVA)
```

```{r blokjegg, echo=T, eval=T, message=F, include=params$answers}
# The regression coefficient for X represents the difference in 
# intercept between the two treatment groups; hence, the estimated
# causal effect here is significantly different from zero: 19.50 
# (se=0.44), p<0.0001.
```

$\blacktriangleright$ Second, estimate an ANCOVA with the gain score (i.e., Y2-Y1) as the outcome variable, X as the grouping variable and Y1 as the covariate (that is: X and Y1 as predictors). Compare the results regarding the estimated causal effect of X from this model to those obtained above.

```{r blokjeh, echo=T, eval=T, message=F, include=params$rcode}
ANCOVA <- lm((Y2-Y1) ~ X + Y1, data=dat1)
summary(ANCOVA)
```

```{r blokjehh, echo=T, eval=T, message=F, include=params$answers}
# The regression coefficient for X is EXACTLY the same across the two analyses.
```


$\blacktriangleright$ Third, estimate the change score model, by regressing the gain score (i.e., Y2-Y1) on X; note this is a regression model with the gain score as the outcome, and only treatment X as its predictor (it may also be recognized by some as an ANOVA on the gain score). Report the parameter (estimate, test, p-value) that is relevant for the causality question.

```{r blokje1j, echo=T, eval=T, message=F, include=params$rcode}
CSA <- lm((Y2-Y1) ~ X, data=dat1)
summary(CSA)
```

```{r blokje1jj, echo=T, eval=T, message=F, include=params$answers}
# The causal effect is here estimated with the regression coefficient for X. 
# Hence, there is evidence that X has a causal effect on the change, as it is 
# estimated to be 19.05 (SE=1.13, p<0.0001). 
```



$\blacktriangleright$ Fourth, estimate the marginal model in which Y2 is regressed on X (hence, Y1 is not included in any way). Report the parameter (estimate, test, p-value) that is relevant for the causal question.

```{r blokje1k, echo=T, eval=T, message=F, include=params$rcode}
Y2onX <- lm((Y2) ~ X, data=dat1)
summary(Y2onX)
```

```{r blokje1kk, echo=T, eval=T, message=F, include=params$answers}
# The causal effect is here estimated with the regression coefficient for X. 
# Hence, there is evidence that X has a causal effect on the change, as it is 
# estimated to be 19.84 (SE=0.90, p<0.0001). 
```

### 2.1.4 Conclusion
When comparing the results, what is your conclusion about the causal effect of treatment on the outcome? 

| Model | Equation | Causal parameter | Estimate (SE) | p-value |
|:----------------------------------|:---------|:----------------:|--------------------------:|--------:|
| ANCOVA | $Y_{2i} = \beta_0 + \beta_1 X_i + \beta_2 Y_{1i} + e_i$               | $\beta_1$  | 19.50 (0.44) | <0.0001 |
| ANCOVA on change score | $Y_{2i} - Y_{1i} = \beta_0 + \beta_1 X_i + (\beta_2-1) Y_{1i} + e_i$  | $\beta_1$  | 19.50 (0.44) | <0.0001  |
| Change score analysis | $Y_{2i} - Y_{1i} = \gamma_0 + \gamma_1 X_i + e_i$                     | $\gamma_1$ | 19.05 (1.13) | <0.0001 |
| Marignal analysis | $Y_{2i} = \phi_0 + \phi_1 X_i + e_i$                                  | $\phi_1$   | 19.84 (0.90) | <0.0001 |




First, we see that doing an ANCOVA with the post-test (Y2) and an ANCOVA with the gain score (Y2-Y1) as the outcome both lead to the exact same estimate of the causal effect. This is in agreement with the expressions for the ANCOVA model that we found in the lecture.

Second, the other two approaches (change score analysis, and marginal model), lead to slightly different estimates (from the ANCOVA and each other). But the substantive conclusions are all the same. This is because Y1 and X are unrelated (as would be the case in an RCT due to random assignment).


## 2.2 Existing treatment groups
A second scenario that we consider, is when there are pre-existing treatment groups. One possible example of such a scenario is the one described by Lord, where the groups were male versus female students. 

The DAG that represents such a scenario looks like this:

![Fig.2a: DAG of pre-existing groups](ANCOVA_DAG3.jpg){width=25%}


The DAG for this scenario which also includes the gain score looks like this:

![Fig.2b: DAG of pre-existing groups with change score included](CHANGE_DAG3.jpg){width=25%}

### 2.2.1 Simulate data
Again, to simulate Y2, we make use of the regression model:
$Y_{2i} = b_0 + b_1 X_i + b_2 Y_{1i} + e_{2i}.$

However, Y1 is now simulated in a different way: Its values will depend on X.

$\blacktriangleright$ Simulate the data using the code below. 

```{r blokje2a, echo=T, eval=T, message=F}
set.seed(268) # set the seed for comparison

N <- 500		  # total sample size

# For X:
X <- rbinom(N,1,0.5)  # simulate treatment

# For Y1:
mY10.true <- 100      # mean of non-treatment group at pretest
mY11.true <- 130      # mean of treatment group at pretest
sdY1 <- 15            # within-group sd at pretest (both groups)

# Note that when X=1, the first part will be zero; if X=0, the second
# part is zero; so this ensures that for each individual only one part
# remains when simulating Y1:
Y1 <- (1-X)*rnorm(N, mY10.true, sdY1) + X*rnorm(N, mY11.true, sdY1)

# As a result, the two groups now have different means (mY10, mY11) on the pretest.

# For Y2:
b0 <- 70		  # intercept
b1 <- 20		  # causal effect		
b2 <- .4		  # covariate effect
sdE2 <- 5		  # within-group residual sd

Y2 <- b0 + b1*X + b2*Y1 + rnorm(N,0,sdE2)

dat2 <- data.frame(Y1,Y2,X)

```



$\blacktriangleright$ Look at the correlations between X, Y1 and Y2; what can you say about these?

```{r blokje2aa, echo=T, eval=T, message=F, include=params$rcode}
round(cor(dat2),3)
```

```{r blokje2aaa, echo=T, eval=T, message=F, include=params$answers}
# Note that Y1 and Y2 are continuous variables, while X is dichotomous.
# To determine the correlation between these three variables, we can 
# use Pearson's correlations.
#
# It shows that:
# Y1 and Y2 are strongly positively related
# X and Y2 are strongly positively related
# Y1 and X are strongly positively related

```


### 2.2.2 Plot the data
We will consider the same two kind of plots (scatterplot with regression lines, and plot of group means at both measurement occasions).

$\blacktriangleright$ First, plot the post-test Y2 against the pre-test Y1. Use different colors for the two groups. What part of this plot informs you on whether there is evidence for a causal effect when using the ANCOVA model?

```{r blokje2c, echo=T, eval=T, message=F, include=params$rcode}
minY <- min(c(Y1,Y2))
maxY <- max(c(Y1,Y2))
plot(x=dat2$Y1, y=dat2$Y2, col = (dat2$X+1),
	xlim=c(minY,maxY),ylim=c(minY,maxY),
	xlab="Y1", ylab="Y2")

```

```{r blokje2cc, echo=T, eval=T, message=F, include=params$answers}
# A causal effect of X on Y in the ANCOVA model would 
# show up as a difference in intercepts of the groups. 
# This difference is actually b1 used in simulating the data.
```

$\blacktriangleright$ Second, make a plot in which you have the two time points on the x-axis and the means of the two groups separately (i.e., the plot related to the changes score approach). How can this plot be used to determine whether a change score analysis would provide evidence for a causal effect?

```{r blokje2d, echo=T, eval=T, message=F, include=params$rcode}
# Means of:
# - pretest non-treatment group (mY01)
# - pretest treatment group (mY11)
mY10 <- mean(Y1[X==0]) 
mY11 <- mean(Y1[X==1]) 

# Means of:
# - posttest non-treatement group (mY20)
# - posttest treatment group (mY21)
mY20 <- mean(Y2[X==0]) 
mY21 <- mean(Y2[X==1]) 

# Gather means on both occasions per treatment condition
mYX0 <- c(mY10,mY20)
mYX1 <- c(mY11,mY21)

minY <- min(mYX0,mYX1)
maxY <- max(mYX0,mYX1)

plot(c(1,2), mYX0, type="l", 
			xlim=c(0.7,2.3),
			ylim=c(minY-5,maxY+5),xaxt="n",
			xlab="time",
			ylab="Y")
lines(c(1,2),mYX1,col="red")
points(c(1,2),mYX1,pch=19,cex=1.3,col="red")
points(c(1,2),mYX0,pch=19,cex=1.3)
axis(side=1, at=seq(1, 2, by=1))

```

```{r blokje2dd, echo=T, eval=T, message=F, include=params$answers}
# A causal effect of X on Y would show up as a different distance between 
# the two lines at the two occasions. Here there is a very small effect, 
# as the two lines are a bit further apart at the second measurement occasion. 
```

### 2.2.3 Analyze the data
Again, we analyze the data with four models.

$\blacktriangleright$ First, estimate the ANCOVA with Y2 as the outcome, and X and Y1 as the predictors. Report the parameters (estimate, test, p-value) that are relevant for the causality question.

```{r blokje6a, echo=T, eval=T, message=F, include=params$rcode}
ANCOVA <- lm(Y2 ~ X + Y1, data=dat2)
summary(ANCOVA)
```

```{r blokje6aa, echo=T, eval=T, message=F, include=params$answers}
# The regression coefficient for X represents the difference in 
# intercept between the two treatment groups; hence, the estimated
# causal effect here is significantly different from zero: 
# 20.23 (se=0.67), p<0.0001.
```

$\blacktriangleright$ Second, estimate the ANCOVA on on the gain score (i.e., Y2-Y1), with X and Y1 as the predictors. Compare the results to the results obtained with an ANCOVA on Y2. 

```{r blokje6b, echo=T, eval=T, message=F, include=params$rcode}
ANCOVA.CS <- lm((Y2-Y1) ~ X + Y1, data=dat2)
summary(ANCOVA.CS)

```


```{r blokje6bb, echo=T, eval=T, message=F, include=params$answers}
# As we also saw in the RCT scenario, the parameter estimate
# for the effect of X on Y2 and on Y2-Y1 is estimated to be
# exactly the same.
# The model fitted here is: Y2 - Y1 = b0 + b1 X + (b2-1) Y1 + e2
# that is, it gives th same b0 and b1 as the ANCOVA model above
# the ony difference is in the coefficient for Y1; here it is
# b2-1 (where b2 is the b2 from the previous model)

```

$\blacktriangleright$ Third, estimate the change score model by regressing the gain score (Y2-Y1) on X. Report the parameter (estimate, test, p-value) that is relevant for the causality question.

```{r blokje7, echo=T, eval=T, message=F, include=params$rcode}
CSA <- lm((Y2-Y1) ~ X, data=dat2)
summary(CSA)
```


```{r blokje7a, echo=T, eval=T, message=F, include=params$answers}
# The causal effect is here estimated with the regression coefficient for X. 
# It is not significantly different from zero:
# 0.49 (SE=0.96), p=0.61
```

$\blacktriangleright$ Fourth, estimate the marginal model by regressing Y2 on X. Report the parameter (estimate, test, p-value) that is relevant for the causality question.

```{r blokje8, echo=T, eval=T, message=F, include=params$rcode}
Y2onX <- lm(Y2 ~ X, data=dat2)
summary(Y2onX)
```


```{r blokje8a, echo=T, eval=T, message=F, include=params$answers}
# The causal effect is estimated with the regression coefficient for X. 
# It is significant and positive:
# 31.99 (SE=0.68), p<0.0000001
```

### 2.2.4 Conclusion
When comparing the results, what is your conclusion about the causal effect of treatment on the outcome, and what model should be preferred?

| Model | Equation | Causal parameter | Estimate (SE) | p-value |
|:----------------------------------|:---------|:----------------:|--------------------------:|--------:|
| ANCOVA | $Y_{2i} = \beta_0 + \beta_1 X_i + \beta_2 Y_{1i} + e_i$               | $\beta_1$  | 20.23 (0.67) | 0.0001|
| ANCOVA on change score | $Y_{2i} - Y_{1i} = \beta_0 + \beta_1 X_i + (\beta_2-1) Y_{1i} + e_i$  | $\beta_1$  | 20.23 (0.67) | 0.0001  |
| Change score analysis | $Y_{2i} - Y_{1i} = \gamma_0 + \gamma_1 X_i + e_i$                     | $\gamma_1$ | 0.49 (0.96) |0.61 |
| Marignal analysis | $Y_{2i} = \phi_0 + \phi_1 X_i + e_i$                                  | $\phi_1$   | 31.99 (0.68) | 0.0001 |



Clearly, the two ANCOVAs lead to the same estimate of the causal effect; it is estimated to be about 20, which is the parameter used in the regression equation to simulate the data. 

In contrast, the change score model is leading to the conclusion that treatment has no effect, while the marginal model (Y2 regressed on X) results in a causal effect of about 30.

Based on the DAGs, we know that X has a direct effect on Y2, which is obtained with the ANCOVA approach. But there is also an indirect effect, as X affects Y1 which in turn affects Y2 (for the sake of completeness, the latter is equal to b2*(mY11-mY10)). 

The marginal model is helping us to estimate that. Hence, if we are interested in the total causal effect of X on Y2, we should consider the last model. But we can add also the estimate of the direct effect, to make the story richer. 

## 2.3 Assignment excl. based on pre-test
In the previous scenario, we started with creating different groups and then simulated the pre-test. Here, we will start with creating a pretest score Y1, and then create two groups X. The groups will be based on a mean-split, and are thus _fully determined_ by the pre-test score. 

An example of such a scenario is when people do a test (e.g., a test of their soccer skills), and all indiduals that score above a certain threshold are then given the treatment (i.e., assigned to X=1, which consists of two additional training sessions per week), whereas the people who score below the threshold are not (i.e., they are assigned to X=0); then, at the end of the year, you measure them agian (i.e., their soccer skills), and you want to determine whether the treatment (additional trainings) had a beneficial effect. 

The DAG of this scenario looks like this:

![Fig.3a: DAG of groups based on pre-test score](ANCOVA_DAG2.jpg){width=25%}

The DAG can be extended with the gain score in it to look like this:

![Fig.3b: DAG of groups based on pre-test score](CHANGE_DAG2.jpg){width=25%}



### 2.3.1 Simulate data
We start with simulating the data. For this, we first need to simulate Y1, which then determines X. Then we can simulate Y2. To make it comparable to the previous example, we use the mean and standard deviation from the Y1 in the previous scenario.Furthermore, when creating Y2, we will make use of the same b0, b1, b2 and residual standard deviation as in the scenario above.

$\blacktriangleright$ Use the following code to simulate the data. 

```{r blokje10, echo=T, eval=T, message=F}
sd.pre <- sd(Y1)			      # determine total sd of Y1 in simulations above
mean.pre <- mean(Y1)		    # determine total mean of Y1 in sims above

# create a new pre-test score with the same mean and variance
Y1 <- rnorm(N,mean.pre,sd.pre)

# Use a mean split for Y1 to create two groups:
X <- rep(0,N)
X [Y1>mean(Y1)] <- 1        # Treatment groups based on mean split of pretest

# Simulate Y2
Y2 <- b0 + b1*X + b2*Y1 + rnorm(N,0,sdE2)

dat3 <- data.frame(Y1,Y2,X)
```

### 2.3.2 Plot the data
$\blacktriangleright$ First, make the pre-post test plot again and describe what this teaches us.

```{r blokje11, echo=T, eval=T, message=F, include=params$rcode}
minY <- min(c(Y1,Y2))
maxY <- max(c(Y1,Y2))
plot(x=dat3$Y1, y=dat3$Y2, col = (dat3$X+1),
	xlim=c(minY,maxY),ylim=c(minY,maxY),
	xlab="Y1", ylab="Y2")
```

```{r blokje11a, echo=T, eval=T, message=F, include=params$answers}
# A causal effect of X on Y according to an ANCOVA model shows up
# as a difference in intercept for the two groups. 
# This difference is actually b1 used in simulating the data.
#
# Note further that the stark difference on the pretest score 
# does not mean there are different groups on the pretest; in fact,
# when the data were created, there were no groups yet; hence, 
# seeing a difference on the pretest, does not inform us whether
# Y1 caused X or X caused Y1! 
```

$\blacktriangleright$ Second, make the plot of the means.

```{r blokje12, echo=T, eval=T, message=F, include=params$rcode}
# Determine means at each occasion for each group
mY10 <- mean(Y1[X==0])
mY11 <- mean(Y1[X==1])
mY20 <- mean(Y2[X==0]) 
mY21 <- mean(Y2[X==1]) 

# Gather means on both occasions per treatment condition
mY0 <- c(mY10,mY20)
mY1 <- c(mY11,mY21)

minY <- min(mY0,mY1)
maxY <- max(mY0,mY1)

plot(c(1,2), mY0, type="l", 
			xlim=c(0.7,2.3),
			ylim=c(minY-5,maxY+5),xaxt="n",
			xlab="time",
			ylab="Y")
lines(c(1,2),mY1,col="red")
points(c(1,2),mY1,pch=19,cex=1.3,col="red")
points(c(1,2),mY0,pch=19,cex=1.3)
axis(side=1, at=seq(1, 2, by=1))

```

```{r blokje12a, echo=T, eval=T, message=F, include=params$answers}
# This plot helps to see what we will get from running a change score model:
# A causual effect of X on Y would show up as a different distance between 
# the two lines at the two occasions. Here there the lines seem parallel, which
# would imply there is no causal effect of X on the gain. 
```

### 2.3.3 Analyze the data
$\blacktriangleright$ First, estimate the ANCOVA model with Y2 as the outcome, and X and Y1 as its predictors. What conclusion can you draw about the causal effect based on this analysis?

```{r blokje13, echo=T, eval=T, message=F, include=params$rcode}
ANCOVA <- lm(Y2 ~ X + Y1, data=dat3)
summary(ANCOVA)
```


```{r blokje13a, echo=T, eval=T, message=F, include=params$answers}
# The regression coefficient for X represents the difference in 
# intercept between the two treatment groups; hence, the estimated
# causal effect here is significantly different from zero and positive:
# 19.24 (SE=0.72), p<0.000001
# which is close to the b1=20 with which we simulated.
```

$\blacktriangleright$ Second, estimate the ANCOVA with teh gain score Y2-Y1 as the outcome and Y1 and X as the predictors. What conclusion can you draw about the causal effect based on this analysis?

```{r blokje13b, echo=T, eval=T, message=F, include=params$rcode}
ANCOVA.CS <- lm((Y2-Y1) ~ X + Y1, data=dat3)
summary(ANCOVA.CS)
```

```{r blokje13bb, echo=T, eval=T, message=F, include=params$answers}
# The regression coefficient for X in this model is 
# exactly the same as that in the previous model
# (as we would expect based on the analytic results 
# we discussed in the lecture).

```

$\blacktriangleright$ Third, estimate the change score model by regressing Y2-Y1 on X. What conclusion can you draw based on this analysis? 

```{r blokje14, echo=T, eval=T, message=F, include=params$rcode}
CSA <- lm((Y2-Y1) ~ X, data=dat3)
summary(CSA)
```

```{r blokje14a, echo=T, eval=T, message=F, include=params$answers}
# The causal effect is here estimated with the regression coefficient for X;
# it is not significantly different from zero:
#  -0.37 (SE=0.80), p=0.64
# which would lead us to conclude that the treatment has 
# no effect on the change.
```

$\blacktriangleright$ Fourth, estimate the marginal model with Y2 as the outcome and X as the predictor. What conclusion can you draw based on this analysis? 

```{r blokje14b, echo=T, eval=T, message=F, include=params$rcode}
Y2onX <- lm(Y2 ~ X, data=dat3)
summary(Y2onX)
```

```{r blokje14bb, echo=T, eval=T, message=F, include=params$answers}
# The causal effect is estimated with the regression coefficient for X;
# here it is significant and positive:
# 32.76 (SE=0.63), p<0.000001
# Hence, treatment has a positive effect on Y2, according to this analysis.
```

### 2.3.4 Conclusion
When comparing the results, what is your conclusion about the causal effect of X, and which analysis should be preferred?

| Model | Equation | Causal parameter | Estimate (SE) | p-value |
|:----------------------------------|:---------|:----------------:|--------------------------:|--------:|
| ANCOVA | $Y_{2i} = \beta_0 + \beta_1 X_i + \beta_2 Y_{1i} + e_i$               | $\beta_1$  | 19.24 (0.72) | <0.0001| 0.0001|
| ANCOVA on change score | $Y_{2i} - Y_{1i} = \beta_0 + \beta_1 X_i + (\beta_2-1) Y_{1i} + e_i$  | $\beta_1$  | 19.24 (0.72) | <0.0001 |
| Change score analysis | $Y_{2i} - Y_{1i} = \gamma_0 + \gamma_1 X_i + e_i$                     | $\gamma_1$ | -0.37 (0.80) | 0.64 |
| Marignal analysis | $Y_{2i} = \phi_0 + \phi_1 X_i + e_i$                                  | $\phi_1$   | 32.76 (0.63) | 0.0001 |


Again, the first two models (ANCOVA and ANCOVA on the change score) lead to the exact same estimate. The other two models lead to different estimates: the third model leads to the conclusion that there is no causal effect, whereas the last model leads to the conclusion that there is a larger positive effect than that obtained with the ANCOVA model. In fact, these results are very similar to the results we obtained with the data in the previous scenario, which illustrates that they are not so informative by themselves.

To decide which analysis is informative if we are interested in obtaining an estimate of the ACE, we should use the DAGs: then, we would conclude that the ANCOVA model is the correct approach here, because Y1 is a confounder (X <- Y1 -> Y2) for which we need to adjust. Note however that it is a very specific scenario, in which there is no overlap between the groups on the covariate (which, btw, would be considered a violation of positivity in the Rubin causal framework!). 


## 2.4 Assignment partly based on pre-test
In the previous scenario, we created different groups using a cut-off for the pre-test. As a result, there was no overlap between the groups on the pretest, which was clearly visible in the first plot (Y2 plotted against Y1) that we made. 

Now, we will create data in which the group assignment is only _partly_ based on the pre-test score. Hence, there should be partial overlap between the two groups on the pre-test. The DAGs for this scenario are the same as the ones for the previous scenario.

### 2.4.1 Simulate data
We start again with creating the pre-test score as we did in the previous scenario. Then we use this variable to assign people to one of two treatment conditions, but now the assignment is not deterministic: There should be overlap between the groups on the pretest score. 

$\blacktriangleright$ We do this using a logistic regression model, using the following code:

```{r blokje16, echo=T, eval=T, message=F}
Y1 <- rnorm(N, mean.pre, sd.pre)  # pre-test

# To create a treatment variable based on this pretest
# but the probability of being treated only partly depends on the pretest
z <- 0.15*(Y1-mean(Y1)) 	      # linear combination 
pr <- 1/(1+exp(-z))         	  # pass through an inv-logit function
X <- rbinom(N,1,pr)      	      # Bernoulli treatment variable

# Now we can create the post-test, like we have done before
Y2 <- b0 + b1*X + b2*Y1 + rnorm(N,0,sdE2)

dat4 <- data.frame(Y1,Y2,X)
```


### 2.4.2 Plot the data
$\blacktriangleright$ First, make the pre-post test plot. 

```{r blokje17, echo=T, eval=T, message=F, include=params$rcode}
minY <- min(c(Y1,Y2))
maxY <- max(c(Y1,Y2))
plot(x=dat4$Y1, y=dat4$Y2, col = (dat4$X+1),
	xlim=c(minY,maxY),ylim=c(minY,maxY),
	xlab="Y1", ylab="Y2")

```


```{r blokje17a, echo=T, eval=T, message=F, include=params$answers}
# A causal effect of X on Y in the ANCOVA model would show up as a difference in intercept
# for the two groups. This difference is actually b1 used in simulating the data.
```

$\blacktriangleright$ Second, make a plot of the means.

```{r blokje18, echo=T, eval=T, message=F, include=params$rcode}
# Compute means at first and second occasion
mY10 <- mean(Y1[X==0])
mY11 <- mean(Y1[X==1])
mY20 <- mean(Y2[X==0])
mY21 <- mean(Y2[X==1])

# Gather means on both occasions per treatment condition
mY0 <- c(mY10,mY20)
mY1 <- c(mY11,mY21)

minY <- min(mY0,mY1)
maxY <- max(mY0,mY1)

plot(c(1,2), mY0, type="l", 
			xlim=c(0.7,2.3),
			ylim=c(minY-5,maxY+5),xaxt="n",
			xlab="time",
			ylab="Y")
lines(c(1,2),mY1,col="red")
points(c(1,2),mY1,pch=19,cex=1.3,col="red")
points(c(1,2),mY0,pch=19,cex=1.3)
axis(side=1, at=seq(1, 2, by=1))

```

```{r blokje18a, echo=T, eval=T, message=F, include=params$answers}
# A causal effect of X on Y in the change score model would show up as a 
# different distance between the two lines at the two occasions. 
# Here there is a very small effect, as the two lines are a bit further
# apart at the second measurement occasion. 
```

### 2.4.3 Analyze the data
$\blacktriangleright$ First, estimate the ANCOVA with Y2 as the outcome, and X and Y1 as its predictors. What conclusion can you draw about the causal effect based on this analysis?

```{r blokje19, echo=T, eval=T, message=F, include=params$rcode}
ANCOVA <- lm(Y2 ~ X + Y1, data=dat4)
summary(ANCOVA)
```

```{r blokje19a, echo=T, eval=T, message=F, include=params$answers}
# The regression coefficient for X represents the difference in 
# intercept between the two treatment groups; hence, the estimated
# causal effect here is significantly different from zero and positive:
# 19.88 (SE=0.61, p<0.000001)
# which is close to the b1=20 with which we simulated.
```

$\blacktriangleright$ Second, estimate the ANCOVA with Y2-Y1 as the outcome and Y1 and X as the predictors. What conclusion can you draw about the causal effect based on this analysis?

```{r blokje20, echo=T, eval=T, message=F, include=params$rcode}
ANCOVA.CS <- lm((Y2-Y1) ~ X + Y1, data=dat4)
summary(ANCOVA.CS)
```

```{r blokje20a, echo=T, eval=T, message=F, include=params$answers}
# The regression coefficient for X in this model is 
# exactly the same as that in the previous model
# (as we would expect based on the analytic results
# we discussed in the lecture).

```

$\blacktriangleright$ Third, estimate the change score model by regressing Y2-Y1 on X. What conclusion can you draw based on this analysis? 

```{r blokje20aa, echo=T, eval=T, message=F, include=params$rcode}
CSA <- lm((Y2-Y1) ~ X, data=dat4)
summary(CSA)
```


```{r blokje20aaa, echo=T, eval=T, message=F, include=params$answers}
# The causal effect is estimated with the regression coefficient for X;
# it is significantly different from zero, but it is much smaller than
# the estimate from the ANCOVA model above:
# 2.02 (SE=0.96, p=0.0356)
# It would lead us to conclude that the treatment has a small positive
# effect on the change.
```

$\blacktriangleright$ Fourth, estimate the marginal model with Y2 as the outcome and X as the predictor. What conclusion can you draw based on this marginal? 

```{r blokje20b, echo=T, eval=T, message=F, include=params$rcode}
Y2onX <- lm(Y2 ~ X, data=dat4)
summary(Y2onX)
```

```{r blokje20bb, echo=T, eval=T, message=F, include=params$answers}
# The causal effect is estimated with the regression coefficient for X;
# here it is significant and positive:
# 31.82 (SE=0.72, p<0.000001)
# Hence, treatment has a large positive effect on Y2, according to this model.
```

### 2.4.4 Conclusion
When comparing the results, what is your conclusion? How can a DAG help to decide which model to use?

| Model | Equation | Causal parameter | Estimate (SE) | p-value |
|:----------------------------------|:---------|:----------------:|--------------------------:|--------:|
| ANCOVA | $Y_{2i} = \beta_0 + \beta_1 X_i + \beta_2 Y_{1i} + e_i$               | $\beta_1$  | 19.88 (0.61) | <0.0001 | 0.0001|
| ANCOVA on change score | $Y_{2i} - Y_{1i} = \beta_0 + \beta_1 X_i + (\beta_2-1) Y_{1i} + e_i$  | $\beta_1$  | 19.88 (0.61) | <0.0001 |
| Change score analysis | $Y_{2i} - Y_{1i} = \gamma_0 + \gamma_1 X_i + e_i$                     | $\gamma_1$ | 2.02 (0.96) | 0.0356 |
| Marignal analysis | $Y_{2i} = \phi_0 + \phi_1 X_i + e_i$                                  | $\phi_1$   | 31.82 (0.72) | 0.0001 |

Again, the two ANCOVAs (with Y2 or Y2-Y1 as the outcome) lead to the same estimate of the causal effect. The change score model (with only X as a predictor of Y2-Y1) gives a much smaller, yet significant positive effect. Finally, the marginal model (With Y2 regressed on X) results in a very large significant positive effect. 

Based on the DAG, we know that Y1 is a common cause of X and Y2, and therefore we should control for it (i.e., condition on it by including it as a predictor). Hence, the two ANCOVA approaches should be preferred over the other two approaches. 

## 2.5 Overall conclusion
What we have seen in this exercise is that the critical distinction between the four scenarios is the relation between X and Y1. In the RCT they are independent of each other, and in that case, all four analyses lead to the same conclusion. 

In the second scenario, we first created X and based on X we created Y1; hence Y1 is a mediator on the indirect path from X to Y2. Controlling for it (as is done in the ANCOVAs) blocks this path, which results in estimating the direct effect rather than the total effect of X on Y2. The change score analysis (i.e., regressing Y2-Y1 on X) results in estimating the total effect of treatment on change, while the marginal model (i.e., regressing Y2 on X) gives the total effect of treatment on Y2. 

The third and fourth scenario are based on creating Y1 first, and then creating X based on this. In these scenarios, Y1 is a confounder or common cause of X and Y2. This means we should definitely control for it if we want to estimate the causal effect of X on Y2. Hence, the ANCOVA would be the analysis of choice here.  

Note that in the current scenarios, the parameters were chosen such that the ANCOVA model resulted in a positive effect of about 20, whereas the change score model tended to result in a non-significant or vary small effect, and the marginal model resulted in a very large positive effect. However, it is also possible to get very different patterns across these models (e.g., see the examples in the lecture for this).


# 3. R based on Kim and Steiner
The change score model has been advocated as a useful approach when there is unobserved time-invariant confounding. __Kim and Steiner (2019)__ discussed this (see this week's reading materials). To get a sense of how this works, you will simulate data with such a time-invariant confounder, and then analyze the data _without_ this confounder (as if it is unobserved!).

Examples of such an unmeasured time-invariant confounder are for instance ability, when we measure something like math achievement or language skills. But an unmeasured time-invariant confounder can also consist of personality, or genetic or physical factors that influence our repeated measures of for instance attitude, motivation, interests, mood, symptoms (e.g., of depression or cancer), behavior, social interactions, and so on.    


## 3.1 Simulate data
We will consider the following model to simulate data:

![_Fig.4: DAG of repeated measures and unmeasured confounding_](KimSteiner0.jpg){width=25%}


$\blacktriangleright$ To begin, simulate the time-invariant confounder U using a standard normal distribution (i.e., mean of zero and standard deviation of 1) for a sample of 5000 people. (Note that we use a (relatively) large sample size here, to decrease the sampling variance.)


```{r blokje30, echo=T, eval=T, message=F}
set.seed(934) # set the seed for comparison
N <- 5000 # sample size
U <- rnorm(N)  # Unmeasured time-invariant confounder
```


$\blacktriangleright$ Next, create the treatment variable that is dependent on this U.

```{r blokje30a, echo=T, eval=T, message=F}
# Create a treatment variable that is dependent on the unmeasured confounder
z <- -1.1*(U) 	        # linear combination with noise
pr <- 1/(1+exp(-z))         	  # pass through an inv-logit function
X <- rbinom(N,1,pr)      	      # bernoulli treatment variable
cor(X,U)                        # check the correlation

# Create the pre-test and the post-test
Y1 <- 10 + 0.7*U + rnorm(N)
Y2 <- 11 + 0.7*U + 0.5*X + rnorm(N)

dat5 <- data.frame(U,X,Y1,Y2)
round(cor(dat5),2)
```

$\blacktriangleright$ Now create Y1 and Y2; both are dependent on U. Furthermore, Y2 also depends on treatment X.

```{r blokje30b, echo=T, eval=T, message=F}
# Create the pre-test and the post-test
Y1 <- 10 + 0.7*U + rnorm(N)
Y2 <- 11 + 0.7*U + 0.5*X + rnorm(N)

dat5 <- data.frame(U,X,Y1,Y2)
round(cor(dat5),2)
```


$\blacktriangleright$ What is the size of the direct effect of X on Y2? And what is the size of the backdoor path? 

```{r blokje30bb, echo=T, eval=T, message=F, include=params$answers}
# There is a direct effect from X to Y2 of size 0.5.
# There is also a backdoor path, X <- U -> Y2; the size of this path
# is more difficult to determine, because X is not linearly related to U
# as it is a binary variable. Hence, we cannot just multiply the 
# path coefficients along this path, like Pearl (2016) and Kim and Steiner (2019)
# do in their examples. Nevertheless, given the specific parameter values, we
# can see it is a negative relation (as higher values on U result a smaller a
# probability of treatment, while U has a positive effect on Y2).
```

$\blacktriangleright$ Based on your answer to the previous answer, what do you expect to happen when you fail to account for the backdoor path, and just regress Y2 on X? 

```{r blokje30c, echo=T, eval=T, message=F, include=params$answers}
# This will result in a biased estimate of the causal effect of X on Y2
# and the bias will be negative, meaning: we will underestimate the effect
# of the treatment X on the outcome Y2. Whether the effect of X on Y2 is
# then estimated to be positive or negative, depends on whether the size
# of the backdoor path is larger or smaller in absolute value than the 
# true causal effect of X on Y2 (which is 0.5).
```



## 3.2 Analyze the data
We will analyze these data in a variety of ways. Note however that in _none_ of the analyses will we be able to include U, as this variable represents the unobserved confounder. That is, we should think of our observed data to consist _only_ of the variables: treatment X, pre-test Y1, and post-test Y2. 

### 3.2.1 Marginal model
We start with the marginal model, by which we estimate the prima facie effect (see Week 2); this implies we just regress Y2 on X, and assume there is no confounding. 

$\blacktriangleright$ What is the marginal effect of X on Y2? How does this compare to the true effect with which you simulated the data?
```{r blokje31, echo=T, eval=T, message=F, include=params$rcode}
Y2onX <- lm(Y2 ~ X, data=dat5)
summary(Y2onX)
```

```{r blokje31b, echo=T, eval=T, message=F, include=params$answers}
# The effect is estimated to be -0.11 (SE=0.03, p<0.00001). 
# This means that on average individuals who received treatment are estimated to 
# score 0.11 points lower on Y2 than individuals who did not receive treatment.
# This is quite different from the true causal effect of X, which is 0.5.
# The negative bias in estimating this effect is the result of the backdoor path
# X <- U -> Y2, which is negative, and that was not blocked in this analysis.
```

### 3.2.2 ANCOVA model
Run an ANCOVA model with X as the predictor of interest, and Y1 as covariate. As Y1 is a proxy of the unmeasured confounder U, including it as a covariate may help to block the backdoor path $X\leftarrow U \rightarrow Y2$ to some extent.

$\blacktriangleright$ What is the effect of X on Y2 in this approach, how would you describe it to others, and how does this compare to the true value?
```{r blokje32, echo=T, eval=T, message=F, include=params$rcode}
ANCOVA <- lm(Y2 ~ X + Y1, data=dat5)
summary(ANCOVA)
```

```{r blokje32b, echo=T, eval=T, message=F, include=params$answers}
# After conditioning on Y1, the effect of X on Y2 is
# 0.05 (SE=0.03, p=0.09)
# which would lead us to conclude X has no causal effect on Y2. 
# More specifically, we would say that, even though there is a mean
# difference between the treatment groups on Y2 (see previous analysis),
# there is no difference between the groups on Y2 after conditioning on
# pretest score Y1. Hence, if we compare individuals with the same Y1, 
# there is no difference between the groups.
# Compared to the true value, the estimated causal effect is too small;
# yet is is closer than the estimate of the prima facie effect (marginal model). 
```

$\blacktriangleright$ Given the result above, what would you expect the scatter plot of the data (with Y2 plotted against Y1 with different colors for the two groups) to look like? Make this plot. 

```{r blokje32c, echo=T, eval=T, message=F, include=params$rcode}
plot(x=dat5$Y1, y=dat5$Y2, col = (dat5$X+1),
      	xlab="Y1", ylab="Y2")

# Note that in the plot the treatment group (X=1) is in red, and the
# control group (no treatment; X=0) is in black.
```


```{r blokje32cc, echo=T, eval=T, message=F, include=params$answers}
# We would expect to see a slight positive relation between Y2 and Y1 in
# each group, as the effect of Y1 on Y2 is estimated to be positive.
# If we would add a regression line for each group to that plot, we would 
# NOT expect to see a difference in the intercepts of the groups (as the
# effect of X represents the difference in intercept, and it is very close
# to zero). 

```


### 3.2.3 Change score model
According to Kim and Steiner (2016), and many others, this model should result in an unbiased estimate of the effect of X on Y2. The DAG for this approach is shown below.


![_Fig.4: DAG of repeated measures and unmeasured confounding_](KimSteiner0b.jpg){width=25%}

$\blacktriangleright$ Use the DAG to explain why the causal effect of X on the gain score G is the same as the effect of the X on Y2.

```{r blokje33, echo=T, eval=T, message=F, include=params$answers}
# The effect of X on Y2 is tau.
# The effect of X on G is tau*1=tau; hence, it is the same.
```

$\blacktriangleright$ Run the change score model, with the gain score G=Y2-Y1 as the outcome variable and X as the predictor. What effect do you find, and how does this compare to the true causal effect of X on Y2?

```{r blokje33a, echo=T, eval=T, message=F, include=params$rcode}
CSA <- lm((Y2-Y1) ~ X, data=dat5)
summary(CSA)
```

```{r blokje33b, echo=T, eval=T, message=F, include=params$answers}
# The causal effect of X on G (and thus on Y2) is estimated to be
# 0.55 (SE=0.04, p<0.000001)
# meaning it is significantly different from zero, and positive.
# It is actually very close to the true value of 0.5 used in simulating
# the data. This confirms the analytical derivation presented by 
# Kim and Steiner, that showed that in this approach, the two paths
# X <- U -> Y1 -> G
# X <- U -> Y2 -> G
# cancel each other out, IF the effect of U on Y1 is the same as the 
# effect of U on Y2 (referred to as common trend or time-invariant 
# confounding assumption).
```

$\blacktriangleright$ Based on the results, do you know what to expect if you would make a plot of the means of the groups at pre-test and at post-test? Make the plot, and compare this to your expectations. 

```{r blokje33c, echo=T, eval=T, message=F, include=params$answers}
# X has a positive effect on G; this means that the two lines will not
# run parallel (there will be a difference in the differences between the
# two groups at the two occasions). 
# Whether the control group increases, decreases, or remains stable over time
# can be seen from the intercept of the gain score, which is:  
# 1.01 (SE=0.03, p<0.000001)
# which indicates that there is (on average) an increase of 1.01 in the 
# control group from the pretest to the posttest.
# Hence, we know that the treatment group will increase by 1.01 + 0.49 = 1.50
# from the pre-test to the post-test. 
```


```{r blokje33d, echo=T, eval=T, message=F, include=params$rcode}
# Compute means at first and second occasion
mY10 <- mean(Y1[X==0])
mY11 <- mean(Y1[X==1])
mY20 <- mean(Y2[X==0])
mY21 <- mean(Y2[X==1])

# Gather means on both occasions per treatment condition
mY0 <- c(mY10,mY20)
mY1 <- c(mY11,mY21)
minY <- min(Y1,Y2)
maxY <- max(Y1,Y2)

# Gather means on both occasions per treatment condition
plot(c(1,2), mY0, type="l", 
			xlim=c(0.7,2.3),
			ylim=c(minY,maxY),xaxt="n",
			xlab="time",
			ylab="Y")
lines(c(1,2),mY1,col="red")
points(c(1,2),mY1,pch=19,cex=1.3,col="red")
points(c(1,2),mY0,pch=19,cex=1.3)
axis(side=1, at=seq(1, 2, by=1))

# Note again in this plot, the treatment group (X=1) is in red, 
# and the control group (no treatment; X=0) is in black.
```

$\blacktriangleright$ Suppose that Y1 and Y2 are measures of math achievement, U is the unobserved math ability, and X is participation in a math camp. How would you explain the results we found above in substantive terms then?

```{r blokje33f, echo=T, eval=T, message=F, include=params$answers}
# The first analysis shows that on the post-test, those who did not 
# participate in the math camp (X=0) score higher on math achievement
# afterwards (Y2) than those who did participate (X=1).
# However, we also see that individuals who participated in the math 
# camp (X=1, red group) scored lower on math achievement to begin with
# (Y1; you can see this from the means in the last plot at the first
# measurement occasion). 
# We also see that both groups score higher on math achievement on the 
# second occasion than at the first. Moreover, we see that those who
# participated in the math camp (X=1, red group) actually increased more 
# over time than the group that did not participate (X=0, black group).
# Hence, the difference between the groups was reduced as a result of
# the intervention. 
```

## 3.3 Conclusion
In this exercise you have gained more experience with the gain score model, and the specific scenario in which it should be preferred over doing an ANCOVA: When there is an unmeasured time-invariant confounder that is a common cause of X, Y1, and Y2, and that has a stable effect on Y1 and Y2. Other assumptions are that there are no other relations between the pre-test Y1 and X, or between Y1 and Y2.   

We have seen that in this scenario, the marginal model (regressing Y2 on X) leads to a biased estimated of the causal effect of X on Y2, as it does not account for the backdoor path X <- U -> Y2.

We have also seen that including Y1 as a proxy of the unmeasured confounder (regressing Y2 on X and Y1; i.e., ANCOVA), only partly removes this effect. 

Here, using a chance score model (regression of gain score Y2-Y1 on X) leads to an unbiased estimate, as the two backdoor paths from X to the gain score G cancel each other out. Kim and Steiner refer to this technique as _off-setting_ these paths, rather than _blocking_ them (which would require conditioning on a variable along the path).  

While this illustrates the elegance of this approach very nicely, we have to realize that the assumptions (mentioned above) are quite strong. Using simulations like these, we could furhter investigate what happens when the effect of U is not stable over time, or when Y1 affects X and/or Y2, or Y1 is affected by X. Such an analysis could help to establish how robust a conclusion (about the strength and sign of the effect of X) is against violations of each of these assumptions.  

# 4. R based on VanderWeele
In this exercise, we will focus on how to investigate the effect of a time-varying binary treatment on a continuous distal outcome, such as discussed by __VanderWeele et al. (2016__; reading materials for this week). Examples of this are:

* church attendence and depressive symptoms in adults

* use of ritalin or physical punishment (by the parents) and behavioral problems (of the child)

* gender quota in a company and the percentage of women working there

* drug use and achademic achievement in adolescents

In all these cases, the treatment is a binary variable (0,1) that may vary over time, and that may affect the final outcome at the end of the study directly or indirectly through shaping earlier realizations of the outcome variable. 

## 4.1 Simulate data and consider the DAG

```{r blokje21, echo=T, eval=T, message=F, include=T}
N <- 1000000
#N <- 10
C <- rnorm(N)

data <- data.frame(C)

# Simulate data for first wave
data$Y1 <- .4*C + rnorm(N)
data$X1 <- rbinom(n = N, size=1, prob = plogis(C))

# Simulate data for the second wave
data$Y2 <- 0.1 * data$Y1 + 0.3 * data$X1 + rnorm(N)
data$X2 <- rbinom(n = N, size=1, prob = 
		plogis(-.8 + 0.2 * data$Y1 + 1 * data$X1))

# Simulate data for the third wave
data$Y3 <- 0.1 * data$Y2 + 0.3 * data$X2 + 
		0.8 * data$Y1 + 0.15 * data$X1 + rnorm(N)
data$X3 <- rbinom(n = N, size=1, prob = 
		plogis(-.8 + 0.2 * data$Y2 + 1 * data$X2 +
			0.1 * data$Y1 + 0.8 * data$X1))

# Simulate the final outcome
data$final.Y <- 0.1 * data$Y3 + 0.3 * data$X3 + 0.8 * data$Y2 + 
			0.15 * data$X2 + rnorm(N)

# Check the datafile
head(data)

```

$\blacktriangleright$ Consider the code provided here, and draw the DAG that is associated with it. 

```{r blokje22, echo=T, eval=T, message=F, include=params$answers}
library(knitr)
include_graphics("TVexposureExercise.jpg")

# Note: The baseline confounder C only has direct effects on X1 and Y1.
# Furthermore, the variables have lag 1 and lag 2 effects.
# Note also that all the arrows towards X variables represent 
# logistic regressions, while the arrows towards Y variables
# represent linear regressions. 
```

$\blacktriangleright$ Indicate which causal paths there are from X1 to Y.final, which do _not_ go through X2 and/or X3, and indicate what the size of each path is.  

```{r blokje22a, echo=T, eval=T, message=F, include=params$answers}
# X1 -> Y2 -> Y3 -> Y.final
0.3*0.1*0.1

# X1 -> Y2 -> Y.final
0.3*0.8

# X1 -> Y3 -> Y.final
0.15*0.1

# Total:
0.3*0.1*0.1 + 0.3*0.8 + 0.15*0.1

```


$\blacktriangleright$ Indicate which causal paths there are from X2 to Y.final, which do _not_ go through X3, and indicate what the size of each path is.  

```{r blokje22b, echo=T, eval=T, message=F, include=params$answers}
# X2 -> Y3 -> Y.final
0.3*0.1

# X2 -> Y.final
0.15

# Total:
0.3*0.1 + 0.15

```

$\blacktriangleright$ Indicate which causal paths there are from X3 to Y.final, and indicate what the size of each path is.  

```{r blokje22c, echo=T, eval=T, message=F, include=params$answers}
# X3 -> Y.final
0.3

# Total:
0.3
```


## 4.2 Estimate two models

We want to estimate the effect of treatment over occasions 1 to 3 (X1 to X3) on the outcome (final.Y). The previous measures of Y (Y1 to Y3) can be referred to as time-vaying covariate. 

$\blacktriangleright$ Discuss for Y1, Y2, and Y3, whether one should or should not control for them here.

```{r blokje23, echo=T, eval=T, message=F, include=params$answers}
# Variable Y1 is only a confounder between treatment and the outcome; 
# hence, we should control for it.
#
# The variable Y2 has two roles in the causal structure:
#
# a) it is a confounder of the relation between X3 and final.Y:
#     X3 <- Y2 -> Y3 -> final.Y
#    This would imply we should control for it 
#    to avoid confounder bias.
#
# b) it is also a mediator for the effect of X1 on final.Y; 
#     X1 -> Y2 -> Y3 -> final.Y and 
#     X1 -> Y2 -> X3 -> final.Y
#     This would imply we should NOT control for it,
#     to avoid overcontrol bias (that is, bias that arises when we 
#     block an indirect path from the cause to the effect)
#     Controlling for Y2 would remove part of the total causal
#     effect X1 has on final.Y.
#
# This is a catch-22.
#
# The variable Y3 is only a mediator in this model:
# X1 -> Y2 -> Y3 -> final.Y
# X1 -> X2 -> Y3 -> final.Y
# X1 -> Y3 -> final.Y
# X2 -> Y3 -> final.Y
# So including Y3 as a covariate will lead to overcontrol
# bias in estimating the effects of X1 and X2 (as it blocks these
# indirect causal paths from X1 and X2 to final.Y); hence,
# we should NOT control for Y3.
```
Before we consider a more sophisticated approach to this problem, we begin with considering two simpler models. Each of these is associated with a specific form of bias in estimating the causal effect of the time-varying exposure on final.Y.

$\blacktriangleright$ Run a regression model with the time-varying covariates included. 

```{r blokje24, echo=T, eval=T, message=F, include=params$rcode}
out1 <- glm(final.Y ~ C + X1 + X2 + X3 + 
                      Y1 + Y2 + Y3, data=data)
summary(out1)
```


```{r blokje24a, echo=T, eval=T, message=F, include=params$answers}
# In this model, we have thr problem of overcontrol bias 
# due to controlLing for mediators Y2 and Y3, which blocks
# causal paths from the time-varying treatment to the outcome.
```



$\blacktriangleright$ Run a regression model without the time-varying covariates.

```{r blokje24aa, echo=T, eval=T, message=F, include=params$rcode}
out2 <- glm(final.Y ~ C + X1 + X2 + X3 + Y1, data=data)
summary(out2)
```


```{r blokje24aaa, echo=T, eval=T, message=F, include=params$answers}
# This model results in confounder bias because it fails
# to control for confounders Y1 and Y2. 
# Note that Y3 is not a counfounder in this model, so 
# omitting it is not associated with confounder bias.
```



$\blacktriangleright$ Compare the results of these two models. 

```{r blokje24b, echo=T, eval=T, message=F, include=params$answers}
# The results for the effect of X1 to X3 on Y.final 
# are quite different across these two models, and they
# also deviate from the actual effects that we computed
# based on the truth above:
#
#       Model 1     Model 2     Truth
# X1     -0.00        0.22      0.258
# X2      0.15        0.14      0.180
# X3      0.30        0.46      0.300
#
# We know that neither model is correct: Model 1 blocks causal 
# paths through Y2 and Y3 (overcontrol bias), while model 2 fails 
# to account for confounding due to Y1 and Y2.
```

$\blacktriangleright$ Seeing the shortcomings of both models, is there a standard regression model that would overcome these? 

```{r blokje24c, echo=T, eval=T, message=F, include=params$answers}
# Since Y1 is only a confounder, it should be included as a covariate 
# (meaning we control for or condition on it). 
# Since Y3 is only a mediator, it should NOT be included as a covariate.
# The problem is with Y2: It is both a confounder (which we should control
# for), AND a mediator (which we should NOT control for). There is no 
# solution for this with standard regression analysis.
```

## 4.3 Marginal structural model

We will now use the marginal strucutral model as described by VanderWeele, Jackson and Li (2016), which was also discussed in the lecture.

$\blacktriangleright$ First, compute the propensity score (i.e., the probability of receiving treatment) at wave 1, wave 2, and 3. For each of these, you should include all prior versions of X and the covariate Y, and all time-invariant (or baseline) covariates (here C). 

```{r blokje25, echo=T, eval=T, message=F, include=params$rcode}
# Compute the propensity scores at wave 1 
res.X1 <- glm(X1 ~  C, family = binomial(), data = data)
ps1 <- predict(res.X1, type = "response")
data$ps1 <- ps1

# Compute the propensity scores at wave 2 
res.X2 <- glm(X2 ~  C + X1 + Y1, family = binomial(), data = data)
ps2 <- predict(res.X2, type = "response")
data$ps2 <- ps2

# Compute the propensity scores at wave 3 
res.X3 <- glm(X3 ~ C + X1 + X2 + Y1 + Y2, family = binomial(), data = data)
ps3 <- predict(res.X3, type = "response")
data$ps3 <- ps3

```

$\blacktriangleright$ Note that strictly speaking, for this particular model (as shown in the DAG), we would not need to include C to estimate the propensity scores at wave 2 and wave 3; explain why not, and whether it matters that we include it here.

```{r blokje25b, echo=T, eval=T, message=F, include=params$answers}
# C only has direct effects on X1 and Y1; since both are included
# as predictors for the propensity scores at wave 2 and wave 3, the 
# effect of C is already controlled for then. However, it does not 
# create a problem to include it; it does not block a indirect path
# that we would want to remain open (i.e., it is not a mediator), nor
# does it open a path that should remain closed (i.e., it is not a collider).
# Including or not including it for ps2 and ps3 does not make a difference.
```

$\blacktriangleright$ Make histograms for the propensity scores of the treated and the untreated at wave 2 and wave 3. What does this show? 

```{r blokje26, echo=T, eval=T, message=F, include=params$rcode}
# Plot the propensity scores at each wave
M<-matrix(c(1:3),1,3, byrow = FALSE)
layout(M)

for (t in 1:3)
{	k <- subset(data, select = c(paste0("X", t)))
	data.1 <- data[ which(k == 1), ]
	data.0 <- data[ which(k == 0), ]
	ps.t.1 <- subset(data.1, select = c(paste0("ps", t)))
	ps.t.0 <- subset(data.0, select = c(paste0("ps", t)))
	hist0 <- hist(as.numeric(ps.t.1[[1]]), breaks=30, plot=FALSE)
	hist1 <- hist(ps.t.0[[1]], breaks=30, plot=FALSE)
	title <- paste0("Propensity scores at wave ", t)
	plot( hist1, col=rgb(0,0,1,1/4), xlim=c(0,1), 
    		xlab="Propensity score", main=title)  
	plot( hist0, col=rgb(0,1,0,1/4), xlim=c(0,1), add=T) 
}

```


```{r blokje26a, echo=T, eval=T, message=F, include=params$answers}
# It shows that the distribution of the propensity scores for the 
# treated and the untreated overlap well (assumption of positivity),
# at each occasion.
```

$\blacktriangleright$ Next, compute the inverse probability weights at wave 2 and 3: Recall this is $1/\pi_i$ for individuals with $X=1$, and $1/(1-\pi_i)$ for individuals with $X=0$. From these occasion specific weights, compute the overall weight, by taking the product of the occasion specific weights. 


```{r blokje27, echo=T, eval=T, message=F, include=params$rcode}
# Compute the inverse probability weights at wave 2 and 3
data$ipw1 <- ifelse(data$X1==1, 1/data$ps1, 1/(1-data$ps1))
data$ipw2 <- ifelse(data$X2==1, 1/data$ps2, 1/(1-data$ps2))
data$ipw3 <- ifelse(data$X3==1, 1/data$ps3, 1/(1-data$ps3))

# Compute the total ipw
data$ipw.123 <- data$ipw1 * data$ipw2 * data$ipw3 

```

$\blacktriangleright$ Finally, we can run a regression model with final.Y as the outcome variable, and X1, X2, X3 and Y1 as its predictors, using the total weights computed above.

```{r blokje28, echo=T, eval=T, message=F, include=params$rcode}
# Regression with inverse probability weighting
# with X1, X2, X3 and Y1 as its predictors
out3 <- glm(final.Y ~ X1 + X2 + X3 + Y1, 
		weights = ipw.123, data=data)

# Another regression with inverse probability weighting 
# which now also includes the baseline confounder (just 
# for comparison)
out4 <- glm(final.Y ~ C + X1 + X2 + X3 + Y1, 
		weights = ipw.123, data=data)

# Another regression with inverse probability weighting 
# which includes C but not Y1 (as VanderWeele et al. suggests
# in their paper)
out5 <- glm(final.Y ~ C + X1 + X2 + X3, 
		weights = ipw.123, data=data)

summary(out3)
summary(out4)
summary(out5)
```

```{r blokje28a, echo=T, eval=T, message=F, include=params$answers}
# It shows that whether the baseline covariate C is 
# included or not, makes no differences; this is because
# it has already been accounted for when computing the
# inverse probability weights.
#
# Furthermore, when considering the parameter estimates here
# it can be seen these are in fact very close to the true effects
# of X1 on Y.final (all paths together but not the ones through X2 and/or X3),
# of X2 on Y.final (all paths together, but not the one through X3),
# and of X3 on Y.final, which were computed at the start based on the
# parameter values that were used to similate the data (i.e., the Truth).
```


## 4.4 Conclusion
$\blacktriangleright$ Compare the results from the marginal structural model to the results from the other two models. What does this show you?

```{r blokje29, echo=T, eval=T, message=F, include=params$answers}
#       Model 1     Model 2     Model 3     Truth
# X1     -0.00        0.22        0.26      0.258
# X2      0.15        0.14        0.18      0.180
# X3      0.30        0.46        0.30      0.300
#
# Model 1 includes Y2 and Y3 as a covariate; it thus leads to 
# overcontrol bias when estimating the effects of X1 and X2;
# it only shows the direct effects of the
#
# time-varying treatment, not the indirect effects.
# Model 2 does not include Y2 and Y3 in any manner; as a result
# this model leads to confounder bias, since Y2 is a counfouder 
# for the X3 -> final.Y relation.
#
# Model 3 is used to acocunt for confounders, without blocking 
# inidrect paths; thereby it should inform us on the joint treatment
# effect of the time-varying treatment. Its estimates are very
# close to the true values (based on the model parameters that we 
# used to simulated the data with).
```

Why the procedure that was described by VanderWeele at al. (2016) works, and how it accounts for time-dependent confounding without blocking the relevant mediation paths, is not easy to see, or to even get some intuition for. But recall that in Week 2 we learned that by using inverse probability weighting, we create a pseudo-population that, within a certain level of the confounder, has an equal number of individuals in each treatment group. That implied that in this pseudo-population, we have $\pi_i=0.5$ for everyone. This balancing property of inverse probability weighting is therefore a way to mimic an RCT. 

That is what we have been trying to do here as well, but now with a treatment at multiple occasions. In the RCT that we try to mimic here, random assignment would have meant that at each occasion, we flip a coin for each person to determine whether they get treatment or not. Hence, some people get the joint treatment {0,0,0}, or {1,0,0}, or {0,1,0}, etc. There are in total $2^3=8$ different treatment regimes that people can have, and by inverse probability weighting, we try to make each (observed) pattern equally likely: Individuals that are charaterized by a pattern that is underrepresented will be weighted more heavily, than individuals that have a pattern that is more common (and that we thus consider to be overrepresented in our sample). It is thus a way of removing the arrows that point into the treatment nodes, as would be the case in an RCT.   

---
