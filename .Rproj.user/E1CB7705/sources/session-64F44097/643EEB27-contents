---
title: "Social Behaviour Dynamics: Week 8 (lab 7)"
author: "Ellen Hamaker"
date: "ADS, 2021-2022"
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output:
  html_document:
    highlight: default
    theme: paper
    toc: yes
    toc_float: yes
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: '5'
params:
  rcode: true
  answers: true
---
# 1. Readings 

We make use of the online book _Forecasting: Principles and Pracice (3rd edition)_ by R. J. Hyndman and G. Athanasopoulos (https://otexts.com/fpp3/). There is an accompanying R-package for this book called fpp3 that we will use in the R-exercises below.

Chapter 1: Getting started

* 1.4 https://otexts.com/fpp3/data-methods.html
* 1.6 https://otexts.com/fpp3/basic-steps.html

Chapter 2: Time series graphics

* 2.1 https://otexts.com/fpp3/tsibbles.html
* 2.2 https://otexts.com/fpp3/time-plots.html
* 2.3 https://otexts.com/fpp3/tspatterns.html
* 2.8 https://otexts.com/fpp3/acf.html
* 2.9 https://otexts.com/fpp3/wn.html

Chapter 5: The forecaster's toolbox

* 5.3 https://otexts.com/fpp3/residuals.html
* 5.4 https://otexts.com/fpp3/diagnostics.html (you can skip the second half about tests)
* 5.10 https://otexts.com/fpp3/tscv.html

Chapter 9: ARIMA models

* Intro: https://otexts.com/fpp3/arima.html
* 9.1: https://otexts.com/fpp3/stationarity.html
* 9.2: https://otexts.com/fpp3/backshift.html
* 9.3: https://otexts.com/fpp3/AR.html
* 9.4: https://otexts.com/fpp3/MA.html
* 9.5: https://otexts.com/fpp3/non-seasonal-arima.html
* 9.6: https://otexts.com/fpp3/arima-estimation.html
* 9.7: https://otexts.com/fpp3/arima-r.html
* 9.8: https://otexts.com/fpp3/arima-forecasting.html
* 9.9: https://otexts.com/fpp3/seasonal-arima.html

__Note__: This is almost the entire chapter; you do not have to understand everything or study everything in great detail, but the later parts require some understanding of the earlier subsections.

Chapter 10: Dynamic regression models

* Intro: https://otexts.com/fpp3/dynamic.html
* 10.1: https://otexts.com/fpp3/estimation.html
* 10.2: https://otexts.com/fpp3/regarima.html
* 10.3: https://otexts.com/fpp3/forecasting.html (only the first example)
* 10.6: https://otexts.com/fpp3/lagged-predictors.html




$~$

# 2. R: Dynamic regression

## 2.0 R-packages
Throughout this practical you will make use of of the R-package fpp3. 

```{r blokje0, echo=T, eval=T, message=F}
# install.packages("fpp3")
# install.packages("expsmooth")
library(fpp3)
library(tseries)
library(expsmooth)
```


$~$

## 2.1. Simulate data
In practice, you have observed data, and you have to try to find out what patterns are present in these data. To develop a better understanding of the different techniques and components that can be used in modeling the structure in time series data, we begin with an exercise in which you simulate data in steps and see what the data at the various intermediate stages look like. 

### 2.1.1 Start with white noise
We start with creating a series of normally distributed white noise. White noise is characterized by acomplete absence of any structure over time: Every observation is independent of the other observations.

$\blacktriangleright$ Use the function rnorm() to simulate a series of 1000 observations, with a mean of zero and a standard deviation of 1.

```{r blokje1a, echo=T, eval=T, message=F, include=params$rcode}
# Note that the default for rnorm() is a mean of zero and sd of 1
# Hence, we only have to indicate the number of observations:
set.seed(512)
u <- rnorm(1000)
```

$\blacktriangleright$ You can use the function plot.ts() to get a sequence plot of the data. Are there any patterns visible in these data? 

```{r blokje2, echo=T, eval=T, message=F, include=params$rcode}
plot.ts(u)
```

```{r blokje2b, echo=T, eval=T, message=F, include=params$answers}
# There is no pattern; this is what we would expect, as the 
# data are entirely random (i.e., independently and identically distributed).
```

$\blacktriangleright$ A second useful way to visualize the data, is by plotting the autocorrelation function. You can use the function acf() for this. Are there specific features to notice about the ACF of these data? 

```{r blokje2c, echo=T, eval=T, message=F, include=params$rcode}
acf(u)
```


```{r blokje2d, echo=T, eval=T, message=F, include=params$answers}
# The autocorrelations show that all the autocorrelations
# are close to zero (except for the autocorrelation at lag 0, 
# which is 1, because that is the correlation between a variable
# and itself).
```



### 2.1.2 Create an ARMA(1,1)
With the white noise series we create, we will now create an ARMA(1,1):

$$ z_t = \phi z_{t-1} + u_t + \theta u_{t-1}$$

Let's use $\phi=0.6$ and $\theta=-0.3$. 

$\blacktriangleright$ Create the ARMA(1,1) series. 

```{r blokje3c, echo=T, eval=T, message=F, include=params$rcode}
z <- rep(NA,1000)
z[1] <- u[1]
for(t in 2:1000){
  z[t] = 0.6*z[t-1] + u[t] - 0.3*u[t-1]
}
```

$\blacktriangleright$ Make a sequence plot and a plot of the ACF of the new series $z_t$. What does this show you? Is there reason to believe the series are non-stationary?

```{r blokje2a, echo=T, eval=T, message=F, include=params$rcode}
plot.ts(z)
acf(z)
```

```{r blokje2aa, echo=T, eval=T, message=F, include=params$answers}
# The sequence plot shows that the fluctuations up and down
# are a bit less random than for the white noise series u_t.
# This aspect is easier to see when considering the ACF: Here
# we see that the autocorrelations at lags 1, 2, and 3 are
# significantly larger than zero; this implies there are 
# sequential dependencies in these data, which makes z_t 
# somewhat predictable from preceding observations (e.g., z_t-1).
# There are no signs of a trend or a unit root process, or
# of a changing variance; hence, the series seem stationary.
```

$\blacktriangleright$ Perform the Dickey-Fuller test using adf.test(). What can you conclude based on this?

```{r blokje2ab, echo=T, eval=T, message=F, include=params$rcode}
adf.test(z)
```


```{r blokje2abb, echo=T, eval=T, message=F, include=params$answers}
# The Dickey-Fuller test is used to determine whether or not
# there is a unit root process; if this is the case, we can/should
# difference the data to get rid of it.
# The H0 (null=hypothesis) of this test is that there is a unit
# root process; the alternative is that there is NO unit root
# process. 
# Here the p-value obtained is very small, which would make us
# reject H0; hence, we conclude there is no evidence for a unit
# root process (i.e., there is no need to difference these data).
```

$\blacktriangleright$ To be able to use fpp3, we have to convert our data to a tsibble object. To this end, we make use of the following code.    

```{r blokje3aa, echo=T, eval=T, message=F}
a <- c(1:1000)
dat <- tibble(date=a,z=z)
dat <- as_tsibble(dat, index=date)
```


$\blacktriangleright$ We can now fit an ARIMA model to these data, and see what model is selected. To this end, use the code below. What is the result? 

```{r blokje3aaa, echo=T, eval=T, message=F}
dat %>% model(ARIMA(z)) -> fit_z
report(fit_z)
gg_tsresiduals(fit_z)
```


$\blacktriangleright$ Above, we let R determine what the order of our ARIMA $(p,d,q)$ had to be. However, since we actually simulated the data, we know that it is an ARIMA$(1,0,1)$. If this is not the model that R slected, you can force R to estimate it by specifying the orders $p$, $d$, and $q$, using the following code:

```{r blokje3baa, echo=T, eval=T, message=F}
dat %>% model(ARIMA(z ~ pdq(p=1, d=0, q=1))) -> fit2_z
report(fit2_z)
gg_tsresiduals(fit2_z)
```

### 2.1.3 Create an ARIMA (1,1,1)
Above we ahve created $z_t$ which is an ARIMA$(1,0,1)$. Now we will create $\eta_t$ which is an ARIMA$(1,1,1)$. Note that 

$$\eta_t - \eta_{t-1} = z_t$$

and therefore

$$\eta_t = \eta_{t-1} + z_t$$

$\blacktriangleright$ Create the series $\eta_t$ using the series $z_t$. 

```{r blokje6a, echo=T, eval=T, message=F, include=params$rcode}
eta <- rep(NA,1000)
eta[1] <- z[1]
for(t in 2:1000){
  eta[t] = eta[t-1] + z[t]
  }
```

$\blacktriangleright$ Make a sequence plot and a plot of the ACF of the new series $\eta_t$. What does this show you? Is there reason to believe the series are non-stationary?

```{r blokje6aa, echo=T, eval=T, message=F, include=params$rcode}
plot.ts(eta)
acf(eta)
```

```{r blokje6baa, echo=T, eval=T, message=F, include=params$answers}
# The sequence shows the series are not just fluctuating around
# a constant mean; rather, there are some large changes here and
# there, which indicate these data are probably not stationary.
# The autocorrelation plot shows very high autocorrelations, that
# only very slowly diminish as the lag (interval) increases. Again,
# this is a sign of non-stationarity.
```

$\blacktriangleright$ Perform the Dickey-Fuller test using adf(test). What can you conclude based on this?

```{r blokje6ab, echo=T, eval=T, message=F, include=params$rcode}
adf.test(eta)
```


```{r blokje7bb, echo=T, eval=T, message=F, include=params$answers}
# The Dickey-Fuller test is non-significant; this means the H0
# (i.e., this is a unit root process) is NOT rejected. 
# This means that if we would analyze these series, we should
# start with differencing them to see whether they can become
# stationary then, before analyzing them using an ARMA(p,q).
```

### 2.1.4 Create $x_t$ and $y_t$
Now we will create the independent predictor $x_t$. Suppose we are dealing with daily data, and we have $x_t$ to indicate whether it was a week day or a weekend day. 

$\blacktriangleright$ Create the dummy variable $x_t$ for weekend days (starting on a Monday).

```{r blokje5, echo=T, eval=T, message=F, include=params$rcode}
# Create dummy x: 1 for Saturdays and Sundays and 0 otherwise
x <- rep(c(0,0,0,0,0,1,1), length.out=1000)
length(x)
```

$\blacktriangleright$ Now we have everything that is needed to create the series $y_t$:

$$y_t = \beta_0 + \beta_1 x_t + \eta_t$$

Use $\beta_0=100$ and $\beta_1=2$. 

```{r blokje5a, echo=T, eval=T, message=F, include=params$rcode}
y <- 100 + 2*x + eta
```


$\blacktriangleright$ Note that in reality, only $y_t$ and $x_t$ would be observed. To be able to use some of the functions from fpp3 to analyze the observed data, we need to have a tsibble object that contains these observed variables. We can make this with the following code (similar as before):

```{r blokje5ef, echo=T, eval=T, message=F}
# Create the right object for fpp3 first
a <- c(1:1000)
obs.dat <- tibble(date=a, y=y, x=x)
obs.dat <- as_tsibble(obs.dat, index=date)
```


## 2.2 Analyze $y_t$ only
Above, we have create data; this helps to see how the various model components enter the model, and what kind of data features they create. But in reality, of course, we do not know which mechanism actually generated the data. Instead, we have observed data, and we want to find out about more about the underlying structure.

Here, we just have the outcome variable $y_t$ (daily measures on for instance sales, customers, or minutes spent on social media). Note that later we will also consider the predictor $x_t$ (day of the week), but for now we just focus on the outcome variable. 

### 2.2.1 Examine $y_t$
$\blacktriangleright$ Make plots for $y_t$ and perform the Dickey-Fuller test. What do you conclude?

```{r blokje5b, echo=T, eval=T, message=F, include=params$rcode}
plot.ts(y)
acf(y)
adf.test(y)
```


```{r blokje5bb, echo=T, eval=T, message=F, include=params$answers}
# The sequence plot and the ACF show clear indicidations of 
# non-sationarity. 
# The Dickey-Fuller test is non-significant; this means the H0
# (i.e., this is a unit root process) is NOT rejected. 
# Hence, we conclude the series is non-stationary.
```

### 2.2.2 Estimate an ARIMA$(p,d,q)$

$\blacktriangleright$ Start with etimating a univariate ARIMA$(p,d,q)$ model for $y_t$, similar to how this was done in 2.1.2 (let R automatically determine the orders of the various components, that is, $p$, $d$, and $q$). What model is estimated? 

```{r blokje15b, echo=T, eval=T, message=F, include=params$rcode}
fit1 <- obs.dat %>% model(ARIMA(y))
report(fit1)
```


```{r blokje15bb, echo=T, eval=T, message=F, include=params$answers}
# Here an ARIMA(0,1,2) is estimated; note however that the model that
# is estimated is dependent on sample fluctuations (another dataset may
# lead to another model being selected as the best option, and you may 
# have obtained something else here). 
# Specifically, here we got:
#
# y_t - y_t-1 = z_t = u_t + 0.2039*u_t-1  - 0.1671*u_t-2
```

$\blacktriangleright$ Plot the residuals of this model using gg_tsresiduals(). What do you conlcude when looking at these plots?

```{r blokje25b, echo=T, eval=T, message=F, include=params$rcode}
gg_tsresiduals(fit1)
```


```{r blokje25bb, echo=T, eval=T, message=F, include=params$answers}
# The sequence plot seems to show these data are stationary: The mean
# and variance seem to be constant over time.
#
# The ACF shows there is still autocorrelation in these data that has
# not been adequately accounted for with the ARIMA(0,1,2) that was fitted.
# This makes sense, because we know there is a week-pattern in these data
# (due to how we generated the data); in real life you of course do not know
# what process generated the data, but with daily data, you may expect there
# to be a week-pattern, just as you may expect an annual pattern or circadian 
# pattern when you have longer or shorter time spans of your study.
# We will account for this later, through the inclusion of x.
# 
# The histogram looks symmetric and normal; there is no indication there
# that we need to transform the data or look for a different model.
```

$\blacktriangleright$ Using the following code you can extract the residuals, and do a Dickey-Fuller test, to see whether these residuals are stationary. What do you conclude?

```{r blokje35b, echo=T, eval=T, message=F}
# To see the object that was generated, use:
augment(fit1)

# Extract the residuals from this object, and perform the 
# Dickey-Fuller test:
adf.test(augment(fit1)$.resid)
```


```{r blokje35bb, echo=T, eval=T, message=F, include=params$answers}
# This Dickey-Fuller test is signficicant; hence, we can reject the H0 that
# these residuals are nonstationary (in other words, the residuals are stationary
# according to this test; this is in agreement with our conclusion based on the 
# sequence plot of the residuals above). 
#
# Hence, the residuals are stationary, but not white noise.
```


### 2.2.3 Forecast based on ARIMA$(p,d,q)$

$\blacktriangleright$ Based on the model that was estimated, we can make forecasts; with the code below we can obtain forecasts for 1 month (i.e., 30 days) and plot these. What do these forecasts show us? 

```{r blokje15a, echo=T, eval=T, message=F}
fit1 %>% forecast(h=30) %>% autoplot(obs.dat) 
fit1 %>% forecast(h=30) -> k
k
```


```{r blokje15aa, echo=T, eval=T, message=F, include=params$answers}
# The forecasts in the plot seem to show a straight line
# with a prediction interval that widens when we forecast
# further forward in time. The latter is due to the intergrated
# part (i.e., the non-stationarity due to a unit root), which makes
# it increasingly more difficult to make predictions further forward
# in time.
#
# When looking at the elements in k, we see the actual values
# that are used for these forecasts; it shows the normal distribution
# in the third column, which is used to draw the 80% and 95% prediction
# intervals, and the mean of this distribution in the fourth column,
# which is considered the actual forecast (use k$.mean to extract these).
#
# This shows that while the first forecast is 77.6, all following forecasts
# are 77.5. When there is no predictor, and there is no cycle or other 
# predictable trend, the forecasts quickly become a constant, as we see here.
```


## 2.3 Dynamic regression
We will now include the observed variable $x_t$ as a predictore, while allowing for ARIMA $(p,d,q)$ residuals. Based on the univariate analysis above, we may expect to find that $d=1$, but it is not clear what will happen to $p$ and $q$ once we add a prdictor to the model. 

### 2.3.1 Estimate dynamic regression model

$\blacktriangleright$ Use the code below to perform this analysis, and interpret the results. 

```{r blokje5e, echo=T, eval=T, message=F}
# Analyse the data
fit2 <- obs.dat %>% model(ARIMA(y ~ x))
report(fit2)
```

```{r blokje5ee, echo=T, eval=T, message=F, include=params$answers}
# We see that x_t is a positive predictor of y_t, and that 
# the residuals are estimated to be an ARIMA(1,1,1) process; 
# this matches with the data generating mechanism (i.e., how these 
# data were simulated), but note that this will not always be the case 
# for every datafile you generate: It partly depends on sample fluctuations
# (you can try this out by using different seeds to start the data 
# generation).
#
# Specifically, the model we get here is:
# y_t = 2.0349*x_t + eta_t
# Where the differenced residuals: eta_t - eta_t-1 = z_t 
# can be expressed as an ARMA(1,1): 
# z_t - 0.4673*z_t-1 = u_t - 0.1512*u_t-1
#
# Furthermore, note that the estimated parameters are somewhat
# like the true values that were used to generate the data:
#       AR(1)         MA(1)         x
# true  0.6           -0.3          2
# est.  0.47(0.07)    -0.15(0.08)   2.03(0.06)
```

$\blacktriangleright$ Plot the residuals of this model using gg_tsresiduals(). What do you conlcude when looking at these plots?

```{r blokje25d, echo=T, eval=T, message=F, include=params$rcode}
gg_tsresiduals(fit2)
```


```{r blokje25dd, echo=T, eval=T, message=F, include=params$answers}
# The sequence plot seems to show these data are stationary: There mean
# and variance seem to be constant over time.
#
# The ACF shows that all autocorrelations are close to zero, and most are 
# non-significant; here the autocorrelation at lag 18 is significantly 
# different from zero, but it is still very small and thus not (very)
# meaningful. Note also that it may be due to coincidence (it is statistics,
# after all...). Hence, it seems safe to conclude these residuals behave
# as white noise. 
# 
# The histogram looks symmetric and normal; there is no indication there
# that we need to transform the data or look for a different model.
```

$\blacktriangleright$ Extract the residuals, and do a Dickey-Fuller test, to see whether these residuals are stationary. What do you conclude?

```{r blokje35d, echo=T, eval=T, message=F, include=params$rcode}
augment(fit2)
adf.test(augment(fit2)$.resid)
```


```{r blokje35dd, echo=T, eval=T, message=F, include=params$answers}
# This Dickey-Fuller test is signficicant; hence, we can reject the H0 that
# these residuals are nonstationary (in other words, the residuals are stationary
# according to this test; this is in agreement with our conclusion based on the 
# seuqence plot of the residuals above). 
```


### 2.3.2 Forecast based on dynamic regression model
Based on the dynamic regression model that was estimated, we can also make forecasts.  

$\blacktriangleright$ To this end, we first have to make futer values for the predictor $x_t$. Since $x_t$ is a dummy variable that indicates whether it was a weekend day, it is easy to make future values for $x_t$. We simply need to determine on what day of the week our last observation was made, and then create the next 28 entries (to represent 4 weeks). This can be done using the following code:

```{r blokje45d, echo=T, eval=T, message=F}
# First check the last 10 entries of x:
x[991:1000]
# This shows that it ends on a "Saturday" (score 1)
#
# We create the future version of x, starting on "Sunday": 
x_future <- new_data(obs.dat, 28) %>% mutate(x = rep(c(1,0,0,0,0,0,1),4))
x_future
```

IMPORTANT: Note that if $x_t$ is a stochastic variable (i.e., it is not deterministic, like day of the week, but instead is something that fluctuates, such as the temperature, or monthly employment percentage), we can create future values of $x_t$ such as the average value of $x_t$; in that case, we would use: mutate(x = mean(obs.dat$x)). (You may need to use this approach in your individual assignment.)

$\blacktriangleright$ We can now make the forecasts for $y_t$ using these future values of $x_t$. What do these forecasts show us? 

```{r blokje15d, echo=T, eval=T, message=F, include=params$rcode}
# Next, we can make forecasts for y based on this future x, and on the
# parameters of the model that was fitted (i.e., the results contained in fit2).
# To plot these forecasts:
forecast(fit2, new_data = x_future) %>% autoplot(obs.dat) 
#
# To see the actual values of these forecasts:
forecast(fit2, new_data = x_future) -> k2
k2
```


```{r blokje15dd, echo=T, eval=T, message=F, include=params$answers}
# The forecasts in the plot clearly show the week vs weekend pattern,
# even though this pattern was not obvious from the observed data due 
# to the non-stationary element in there. 
#
# By looking at k2, we see that the ARIMA model for the residuals has 
# some effect on the first prediction, but after that, the predictions 
# are entirely determined by x_t.
#
# Furthermore, the prediction interval increases the futhre forward in
# time we try to forecast, due to the unit root component. 
```


#3 R: Dynamic regression with differenced data
Above, we made use of the function ARIMA(), which automatically estimates an ARIMA$(p,d,q)$. When we included $x_t$ as a predictore of $y_t$, we found we had ARIMA$(1,1,1)$ residuals. This model can be expressed as:

$$ y_t = \beta_0 + \beta_1 x_t + \eta_t $$

where differencing the residuals $\eta_t$ results in the stationary series $z_t = \eta_t - \eta_{t-1}$, which can be expressed as an $ARMA(1,1)$

$$ z_t = \phi z_{t-1} + u_t + \theta u_t$$

Another way to specify and estimate this same model, and to get the same results, is by first differencing the predictor $x_t$ and the outcome $y_t$ ourselves, and then do a dynamic regression analysis on these differenced data. This is an approach you will use next week; here we investigate it and compare it to the approach used above.  

## 3.1 Differencing the data
We have seen that the data that were created showed evidence of a unit-root, as the Dickey-Fuller test was non-significant. 

$\blacktriangleright$ You can difference $y_t$ with the diff() function from the base package. Use a lag of 1 (so normal diferencing), and check whether this differenced series is stationary.


```{r blokje25c, echo=T, eval=T, message=F, include=params$rcode}
y.diff <- diff(y, lag=1)
plot.ts(y.diff)
acf(y.diff)
adf.test(y.diff)
```


```{r blokje25cc, echo=T, eval=T, message=F, include=params$answers}
# The sequence plot shows that the differenced series no longer show 
# the typical signs of non-stationarity: We now have fluctuations up 
# and down around a constant with a constant variance around it.
#
# The ACF shows a regular pattern, which clearly indicates this series
# is NOT a white noise process, as there large autocorrelations at 
# particular lags (i.e., there is still structure in the data).
#
# The Dickey-Fuller test is significant, which implies that we can conclude
# that the differenced series are stationary.
#
# Note that this is what we expect, based on how these data were generated.
```


## 3.2 Normal regression with differenced variables

$\blacktriangleright$ Difference the predictor $x_t$ in the same way, and run a normal regression: 

$$ (y_t - y_{t-1}) = b_0 + b_1 (x_t - x_{t-1}) + e_t $$

where $e_t$ is assumed to be a normally distributed white noise series.

Compare the parameter estimate you get for $b_1$ with the regression coefficient you estimated above in the dynamic regression model. What is your conclusion?

```{r blokje85c, echo=T, eval=T, message=F, include=params$rcode}
# Difference the predictor variable:
x.diff <- diff(x, lag=1)

# Perform a regression analysis on the differenced series y and
# the differences series x
summary(lm(y.diff ~ x.diff))

# Results obtained before with dynamic regression
report(fit2)
```

```{r blokje85d, echo=T, eval=T, message=F, include=params$answers}
# The regression coefficient from the normal regression analysis
# with differenced outcome and predictor is 2.0423 (SE=0.0626);
# the regression coefficient from the dynamic regression model 
# that was estimated before is: 2.0349 (SE=0.0550). 
#
# Hence, the parameter estimates are pretty similar, but not the same. 
```

## 3.3 Dynamic regression with differenced variables

$\blacktriangleright$ Next, run the dynamic regression analysis on the differenced outcome using the differenced predictor:

$$ (y_t - y_{t-1}) = b_0 + b_1 (x_t - x_{t-1}) + \eta_t $$
where $\eta_t$ is modeled as an ARIMA process. 

Compare the parameter estimate you get for $b_1$ with the regression coefficient you estimated above in the dynamic regression model. What is your conclusion?

```{r blokje85e, echo=T, eval=T, message=F, include=params$rcode}
# Perform a regression analysis with the differenced series y and x
# and ARIMA residuals
fit3 <- obs.dat %>% model(ARIMA(y.diff ~ x.diff))
report(fit3)

# The results obtained before were:
report(fit2)
```

```{r blokje85cc, echo=T, eval=T, message=F, include=params$answers}
# This shows that we get the exact same parameter estimates for
# the predictor in both analyses: 2.0423 (SE=0.0626).
# Moreover, we also get the exact same estimates for the auotergressive
# parameter (0.4673) and the moving average parameter (-0.1512).
# The standard errors and (log-likelihood) are also exactly identical;
# this implies that the two analyses are equivalent.
```

## 3.4 Conclusion 
The approach we have used here, where we first difference the output and the predictor and then do a dynamic regression analysis on these differenced variables leads to the exact same results as when doing the dynamic regression analysis on the orignial variables. This means these models are equivalent.

This actually matches also the analytical result that was presented in the lecture. 


# 4. R: Seasonal ARIMA
In the first R exercise you simulated data with ARIMA(1,1,1) reiduals. In this exercise we will consider a more realistic situation, in which you have to analyze an existing dataset.

Specificallu, we will make use of the dataset "ukcars" (which is included in the package expsmooth). It contains the quaterly passenger car production (thousands of cars) from the first quarter of 1977 until the first quarter of 2005.

$\blacktriangleright$ Look at the data "ukcars", and describe the structure of this data file. 

```{r blokje1, echo=T, eval=T, message=F, include=params$rcode}
ukcars
```

```{r blokje111a, echo=T, eval=T, message=F, include=params$answers}
# Rows are years, columns are quarters (seasons).
```

$~$

## 4.1 Visualize the data
Before analyzing the data, we consider a few visualizations of the data.

$\blacktriangleright$ First, create a sequence plot of the data. You can use the function autoplot() for this. Are there any patterns visible in these data? 

```{r blokje2x, echo=T, eval=T, message=F, include=params$rcode}
autoplot(ukcars)
```

```{r blokje2xb, echo=T, eval=T, message=F, include=params$answers}
# There are clear trends over time:
# there is a decrease at the beginning,
# followed by an increase, then a sudden dip,
# followed by a partial recovery.
# At a shorter timescale, there also seems
# a seasonal pattern (i.e., an annual period).
# These data are obviously not stationary.
```
$\blacktriangleright$ A second useful way to visualize the data, is by plotting the autocorrelation function. You can use the function acf() for this. Are there specific features to notice about the ACF of these data? 

```{r blokje2xc, echo=T, eval=T, message=F, include=params$rcode}
acf(ukcars)
```


```{r blokje2xd, echo=T, eval=T, message=F, include=params$answers}
# The autocorrelations are quite high and stay like that
# for a long time (i.e., over larger lags as well). 
# This matches the fact that there are obvious long-term
# trends visable in the data (see sequence plot we made 
# above).
#
# Furthermore, there is a peak at lags 4, 8, 12, etc., which
# is consistent with the idea that these data contain a 
# seasonal effect.
```


$\blacktriangleright$ Do a Dickey-Fuller test; what is your conclusion?   

```{r blokje3x, echo=T, eval=T, message=F, include=params$rcode}
adf.test(ukcars)
kpss.test(ukcars)
kpss.test(ukcars, null="Trend")
```

```{r blokje3xc, echo=T, eval=T, message=F, include=params$answers}
# It shows that there may be a seasonal effect, which requires us
# to difference the series using a lag of 4. 
# Note however that the test is significant
# which would mean we reject H0 (non-stationarity). 
```

Instead of trying out various degrees of differencing ourselves, we can let R figure out what the best ARIMA$(p,d,q)$ or seasonal ARIMA$(p,d,q)(P,D,Q)$ is for these data. To this end, we will make use of the ARIMA() function again. 

But first we need to convert our data to an tsiblle object. 

$\blacktriangleright$ Use the function as_tsibble for this. Describe the structure of this object.   

```{r blokje3xa, echo=T, eval=T, message=F, include=params$rcode}
uk.cars <- as_tsibble(ukcars)
uk.cars
```

```{r blokje3xb, echo=T, eval=T, message=F, include=params$answers}
# The object has two columns:
# an index column that represents the time
# and a value column that represents the actual 
# values of the variable over time.
```

## 4.2 SARIMA modeling
A SARIMA model is denoted as SARIMA$(p,d,q)(P,D,Q)[m]$, where the lower case letters indicate the order of regular autoregression (p), differencing (d), and moving averaging (q), while the capitals indicate the seasonal autoregression (P), differencing (D), and moving averaging (Q), with m being the period. If there is a seasonal component here, we may expect this to be 4 (as these are quarterly data). 

$\blacktriangleright$ As before, we use the function ARIMA. What is the order of the SARIMA model that is selected?

```{r blokje6xc, echo=T, eval=T, message=F, include=params$rcode}
fit.cars <- uk.cars %>% model(ARIMA(value)) 
report(fit.cars)
```

```{r blokje6d, echo=T, eval=T, message=F, include=params$answers}
# This shows we get an ARIMA(1,0,1)(1,1,2)[4] 
# Hence, the seasonal period is 4 (which again makes
# sense because we have quarterly data), and the
# model that is selected is based on seasonal 
# differencing (D=4).
# Then the AR and MA orders are both 1 (p and q), 
# and the seasonal AR and MA orders are 1 and 2 (P, and Q).
```

$\blacktriangleright$ For this SARIMA model, make the usual plots of the residuals using gg_tsresiduals(). Is there anything to note or be concerned about? 

```{r blokjex7, echo=T, eval=T, message=F, , include=params$rcode}
gg_tsresiduals(fit2)     
```

```{r blokje7b, echo=T, eval=T, message=F, include=params$answers}
# The residuals do not show any obvious trend anymore.
# The ACF shows all autocorrelations seem to be zero,
# meaning there is no structure in these residuals left.
# The histogram of the residuals looks a little skewed.
```

$\blacktriangleright$ Make forecasts based on this SARIMA model and plot these, using a forecasting hozion of 12. What does this horizon mean, and what do you see?

```{r blokjex8, echo=T, eval=T, message=F, , include=params$rcode}
fit.cars %>% forecast(h=12) %>% autoplot(uk.cars)  
```

```{r blokje8b, echo=T, eval=T, message=F, include=params$answers}
# A forecasting hozizon of 12 means we make predictions 12 
# steps (time points) foreward in time; as these are quarterly
# data, this means we make forecasts for the next 3 years.
#
# The forecasts based on the SARIMA model, show a clear
# seasonal pattern over time. It shows that at first, the
# forecasts are not entirely the same every year, but as 
# we increase the forecasting horizon, the forecasts start
# to follow the exact same pattern every year. Also, the 
# prediction intervals keep widening over time (as a result
# of the non-stationarity).
```

## 4.3 Looking under the hood (OPTIONAL)
When checking the output of the ARIMA function, you could see not only the orders (p,d,q) and (P,D,Q)[m], but also the actual parameter estimates. The model that is selected here is actually quite complicated, as will be shown below. 

Please note that you are free to skip this part; it merely meant for satisfy those of you who really want to see how this works. 

To find the actual expression of this SARIMA model, we begin with focusing on the differencing part. Let as refer to the observed series as $y_t$. We have d=0 and D=1 with m=4, so there is only seasonal differencing. Applying this seasonally differencing to $y_t$, we get $z_t = (1-B^4) y_t = y_t - y_{t-4}$.

Given that $y_t$ is a ARIMA(1,0,1)(1,1,2)[4], the (correctly) differences series $z_t$ is a stationary ARMA(1,1)(1,2)[4] process. This can be written as
$$(1 - \phi B) (1 - \Phi B^4) z_t = (1 + \theta B) (1 + \Theta_1 B^4 + \Theta_2 B^5) u_t$$

The term on the left can be written as:

$$(1 - \phi B - \Phi B^4 + \phi \Phi B^5) z_t = z_t - \phi z_{t-1} - \Phi z_{t-4} + \phi\Phi z_{t-5}$$

The term on the right can be written as:
$$(1 + \theta B + \Theta_1 B^4 + \Theta_2 B^5 + \theta\Theta_1 B^5 + \theta \Theta_2 B^6) u_t 
= u_t + \theta u_{t-1} + \Theta_1 u_{t-4} + \Theta_2 u_{t-5}
    + \theta \Theta_1 u_{t-5} + \theta\Theta_2 u_{t-6}$$
    
Now we can put these together to get

$$z_t - \phi z_{t-1} - \Phi z_{t-4} + \phi\Phi z_{t-5} = u_t + \theta u_{t-1} + \Theta_1 u_{t-4} + \Theta_2 u_{t-5}
    + \theta \Theta_1 u_{t-5} + \theta\Theta_2 u_{t-6}$$
    
Next, we can replace $z_t$ by $y_t - y_{t-4}$, to get

$$(y_t - y_{t-4}) - \phi (y_{t-1} - y_{t-5}) - \Phi (y_{t-4} - y_{t-8})+ \phi\Phi (y_{t-5} - y_{t-9}) = u_t + \theta u_{t-1} + \Theta_1 u_{t-4} + (\Theta_2 + \theta \Theta_1) u_{t-5} + \theta\Theta_2 u_{t-6}$$

Now we can move all terms accept $y_t$ to the right to get

$$y_t = y_{t-4} + \phi (y_{t-1} - y_{t-5}) + \Phi (y_{t-4} - y_{t-8}) - \phi\Phi (y_{t-5} - y_{t-9}) + u_t + \theta u_{t-1} + \Theta_1 u_{t-4} + (\Theta_2 + \theta \Theta_1) u_{t-5} + \theta\Theta_2 u_{t-6}$$

Note that the actual parameters can be found in the output. They are:

$\phi = 0.925$

$\theta = -0.339$

$\Phi = -0.752$

$\Theta_1 = -0.147$

$\Theta_2 = -0.395$

## 4.4 Conclusion
One may pose the question what we can learn from this model (and the expression that was derived above). In general, such complicated SARIMA models are not interpreted in substantive terms, but rather seen as a way to pre-whiten your data; that is, it is a way to get from autocorrelated data to white noise in which there are no further patterns and predictability left. Furthermore, they can be used for forecasting, and they can be compared to other models/techniques using cross-validation. 


---
